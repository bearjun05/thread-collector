"""Threads ìŠ¤í¬ë˜í¼ API ì„œë²„"""
import asyncio
import json
import os
import secrets
import sqlite3
import traceback
from email.utils import format_datetime
import hashlib
import subprocess
from datetime import datetime, timedelta, timezone
from typing import List, Dict, Any, Optional, Literal

from fastapi import FastAPI, HTTPException, Query, Depends, Request
from fastapi import BackgroundTasks
from fastapi.responses import JSONResponse
from fastapi.responses import FileResponse, PlainTextResponse, Response
from fastapi.security import HTTPBasic, HTTPBasicCredentials
from pydantic import BaseModel, Field
from dateutil import parser as date_parser

from scraper import (
    scrape_threads_profile,
    search_threads_users,
    scrape_thread_with_replies,
    scrape_threads_profile_with_replies,
)
from rss_sync import (
    ensure_schema as ensure_rss_schema,
    run_once as rss_run_once,
    LOG_PATH as RSS_LOG_PATH,
    DB_PATH as RSS_DB_PATH,
    refresh_rss_cache_for_account,
    load_accounts,
)
from rss_render import (
    xml_escape as _xml_escape,
    build_description_html as _shared_build_description_html,
    build_content_html as _shared_build_content_html,
    collect_youtube_embeds as _shared_collect_youtube_embeds,
    reply_texts_from_tuples as _reply_texts_from_tuples,
    build_enclosures as _build_enclosures,
    build_media_contents as _build_media_contents,
    build_media_players as _build_media_players,
)
from curation import (
    ensure_schema as ensure_curation_schema,
    get_config as curation_get_config,
    update_config as curation_update_config,
    run_curation_once as curation_run_once,
    list_runs as curation_list_runs,
    list_candidates as curation_list_candidates,
    update_candidate as curation_update_candidate,
    create_candidate_list_once as curation_create_candidate_list_once,
    score_candidates_once as curation_score_candidates_once,
    summarize_candidates_once as curation_summarize_candidates_once,
    preview_run as curation_preview_run,
    approve_all as curation_approve_all,
    reorder_candidates as curation_reorder_candidates,
    publish_run as curation_publish_run,
    latest_ready_run as curation_latest_ready_run,
    latest_publication as curation_latest_publication,
    maybe_auto_publish_due as curation_maybe_auto_publish_due,
    refresh_cache as curation_refresh_cache,
    get_cached_feed as curation_get_cached_feed,
    stats as curation_stats,
    ITEM_FEED as CURATED_ITEM_FEED,
    DIGEST_FEED as CURATED_DIGEST_FEED,
)

# í•œêµ­ í‘œì¤€ ì‹œê°„ëŒ€ (KST = UTC+9)
KST = timezone(timedelta(hours=9))
UTC = timezone.utc

app = FastAPI(
    title="Threads Collector API",
    description="Threads í”„ë¡œí•„ ê²Œì‹œë¬¼ì„ ìŠ¤í¬ë˜í•‘í•˜ëŠ” API ì„œë²„",
    version="0.1.0",
)

security = HTTPBasic()


def _require_admin(credentials: HTTPBasicCredentials) -> None:
    admin_user = os.environ.get("ADMIN_USER", "admin")
    admin_password = os.environ.get("ADMIN_PASSWORD")
    if not admin_password:
        raise HTTPException(status_code=500, detail="ADMIN_PASSWORD is not set")
    if credentials.username != admin_user or credentials.password != admin_password:
        raise HTTPException(status_code=401, detail="Unauthorized")


def _get_db_conn() -> sqlite3.Connection:
    conn = sqlite3.connect(RSS_DB_PATH)
    conn.execute("PRAGMA foreign_keys = ON")
    ensure_rss_schema(conn)
    ensure_curation_schema(conn)
    return conn


def _tail_log(path: str, lines: int = 200) -> str:
    try:
        with open(path, "rb") as f:
            f.seek(0, os.SEEK_END)
            size = f.tell()
            block = 4096
            data = b""
            while size > 0 and data.count(b"\n") <= lines:
                read_size = block if size - block > 0 else size
                f.seek(-read_size, os.SEEK_CUR)
                data = f.read(read_size) + data
                f.seek(-read_size, os.SEEK_CUR)
                size -= read_size
            text = data.decode("utf-8", errors="replace")
            return "\n".join(text.splitlines()[-lines:])
    except FileNotFoundError:
        return ""


def _append_log(message: str) -> None:
    ts = datetime.now(KST).isoformat()
    line = f"[{ts}] {message}\n"
    try:
        with open(RSS_LOG_PATH, "a", encoding="utf-8") as f:
            f.write(line)
    except Exception:
        pass


def _get_schedule(conn: sqlite3.Connection, display_kst: bool = True) -> Dict[str, Any]:
    row = conn.execute(
        "SELECT is_active, interval_minutes, start_time, last_run_at, updated_at FROM rss_schedule WHERE id = 1"
    ).fetchone()
    if not row:
        now = datetime.now(timezone.utc).isoformat()
        conn.execute(
            "INSERT INTO rss_schedule (id, is_active, interval_minutes, start_time, last_run_at, updated_at) "
            "VALUES (1, 1, 30, '09:00', NULL, ?)",
            (now,),
        )
        conn.commit()
        row = (1, 30, "09:00", None, now)
    start_time_utc = row[2]
    start_time_value = start_time_utc
    if display_kst:
        try:
            hh, mm = start_time_utc.split(":")
            kst_dt = datetime(2000, 1, 1, int(hh), int(mm), tzinfo=timezone.utc).astimezone(KST)
            start_time_value = f"{kst_dt.hour:02d}:{kst_dt.minute:02d}"
        except Exception:
            start_time_value = "09:00"
    return {
        "is_active": bool(row[0]),
        "interval_minutes": row[1],
        "start_time": start_time_value,
        "start_time_utc": start_time_utc,
        "last_run_at": row[3],
        "updated_at": row[4],
    }


def _get_rate_limit(conn: sqlite3.Connection) -> Dict[str, Any]:
    row = conn.execute(
        "SELECT window_seconds, max_requests, updated_at FROM rss_rate_limits WHERE id = 1"
    ).fetchone()
    if not row:
        now = datetime.now(timezone.utc).isoformat()
        conn.execute(
            "INSERT INTO rss_rate_limits (id, window_seconds, max_requests, updated_at) "
            "VALUES (1, 300, 60, ?)",
            (now,),
        )
        conn.commit()
        row = (300, 60, now)
    return {
        "window_seconds": row[0],
        "max_requests": row[1],
        "updated_at": row[2],
    }


def _get_cache_policy(conn: sqlite3.Connection) -> Dict[str, Any]:
    row = conn.execute(
        "SELECT enabled, ttl_seconds, updated_at FROM rss_cache_policy WHERE id = 1"
    ).fetchone()
    if not row:
        now = datetime.now(timezone.utc).isoformat()
        conn.execute(
            "INSERT INTO rss_cache_policy (id, enabled, ttl_seconds, updated_at) "
            "VALUES (1, 1, 300, ?)",
            (now,),
        )
        conn.commit()
        row = (1, 300, now)
    return {
        "enabled": bool(row[0]),
        "ttl_seconds": row[1],
        "updated_at": row[2],
    }


def _log_token_request(
    conn: sqlite3.Connection,
    token_id: int,
    username: str,
    request: Request,
    status_code: int,
) -> None:
    try:
        now = datetime.now(timezone.utc).isoformat()
        ip = request.client.host if request.client else None
        ua = request.headers.get("user-agent")
        conn.execute(
            "INSERT INTO rss_token_logs (token_id, username, ip, user_agent, status_code, created_at) "
            "VALUES (?, ?, ?, ?, ?, ?)",
            (token_id, username, ip, ua, status_code, now),
        )
        conn.commit()
    except Exception:
        pass


def _log_invalid_token(
    conn: sqlite3.Connection,
    token_value: str,
    username: Optional[str],
    request: Request,
    status_code: int,
) -> None:
    try:
        now = datetime.now(timezone.utc).isoformat()
        token_hash = hashlib.sha256(token_value.encode("utf-8")).hexdigest()
        ip = request.client.host if request.client else None
        ua = request.headers.get("user-agent")
        conn.execute(
            "INSERT INTO rss_invalid_token_logs (token_hash, username, ip, user_agent, status_code, created_at) "
            "VALUES (?, ?, ?, ?, ?, ?)",
            (token_hash, username, ip, ua, status_code, now),
        )
        conn.commit()
    except Exception:
        pass


def _compute_schedule_times(
    now_utc: datetime, start_time_utc: str, interval_minutes: int
) -> tuple[Optional[datetime], datetime]:
    try:
        hh, mm = start_time_utc.split(":")
        start_today = now_utc.replace(hour=int(hh), minute=int(mm), second=0, microsecond=0)
    except Exception:
        start_today = now_utc.replace(hour=0, minute=0, second=0, microsecond=0)
    if now_utc < start_today:
        return None, start_today
    delta_minutes = int((now_utc - start_today).total_seconds() // 60)
    steps = delta_minutes // max(1, interval_minutes)
    prev = start_today + timedelta(minutes=steps * interval_minutes)
    return prev, prev + timedelta(minutes=interval_minutes)


def _compute_next_run(schedule: Dict[str, Any], start_time_utc: str) -> Optional[str]:
    if not schedule.get("is_active"):
        return None
    now = datetime.now(timezone.utc)
    _, next_dt = _compute_schedule_times(now, start_time_utc, schedule.get("interval_minutes") or 30)
    return next_dt.astimezone(KST).isoformat()


async def _scheduler_loop() -> None:
    lock = asyncio.Lock()
    while True:
        await asyncio.sleep(30)
        try:
            conn = _get_db_conn()
            sched = _get_schedule(conn, display_kst=False)
            conn.close()
            if not sched["is_active"]:
                continue
            last = sched["last_run_at"]
            last_dt = None
            if last:
                try:
                    last_dt = date_parser.parse(last)
                except Exception:
                    last_dt = None
            now = datetime.now(timezone.utc)
            prev_due, _next_due = _compute_schedule_times(
                now, sched.get("start_time_utc") or "00:00", sched.get("interval_minutes") or 30
            )
            if prev_due is None:
                due = False
            else:
                if last_dt:
                    if last_dt.tzinfo is None:
                        last_dt = last_dt.replace(tzinfo=timezone.utc)
                    due = last_dt < prev_due
                else:
                    due = True
            if not due:
                continue
            if lock.locked():
                continue
            async with lock:
                await rss_run_once(None)
                conn = _get_db_conn()
                now_iso = datetime.now(timezone.utc).isoformat()
                conn.execute(
                    "UPDATE rss_schedule SET last_run_at = ?, updated_at = ? WHERE id = 1",
                    (now_iso, now_iso),
                )
                conn.commit()
                conn.close()
        except Exception:
            continue


def _collect_youtube_embeds(root_text: str, replies: List[tuple]) -> List[str]:
    return _shared_collect_youtube_embeds(root_text, _reply_texts_from_tuples(replies))


def _build_description_html(root_text: str, replies: List[tuple]) -> str:
    return _shared_build_description_html(root_text, _reply_texts_from_tuples(replies))

def _build_content_html(
    root_text: str,
    replies: List[tuple],
    root_media_urls: List[str],
    media_urls: List[str],
    youtube_embeds: List[str],
) -> str:
    content_blocks = [{"text": root_text, "media_urls": root_media_urls}]
    for reply in replies or []:
        reply_media_urls: List[str] = []
        if len(reply) > 1:
            reply_media = _parse_media_json(reply[1])
            reply_media_urls = [m.get("url") for m in reply_media if m.get("url")]
        content_blocks.append(
            {
                "text": reply[0] or "",
                "media_urls": reply_media_urls,
            }
        )
    return _shared_build_content_html(
        root_text,
        _reply_texts_from_tuples(replies),
        media_urls,
        youtube_embeds,
        content_blocks=content_blocks,
    )


def _build_html_preview(title: str, link: str, items: List[Dict[str, Any]]) -> str:
    cards = []
    for it in items:
        cards.append(
            f"<article class=\"item\">"
            f"<h2><a href=\"{_xml_escape(it['link'])}\">{_xml_escape(it['title'])}</a></h2>"
            f"<div class=\"meta\">{_xml_escape(it['pubDate'])}</div>"
            f"<div class=\"content\">{it['content_html']}</div>"
            f"</article>"
        )
    return (
        "<!doctype html>"
        "<html><head><meta charset=\"utf-8\" />"
        f"<title>{_xml_escape(title)}</title>"
        "<style>"
        "body{font-family:-apple-system,BlinkMacSystemFont,sans-serif;background:#f7f4ee;color:#1a1a1a;padding:24px;}"
        ".item{background:#fff;border:1px solid #e5dcc8;border-radius:12px;padding:16px;margin-bottom:16px;}"
        ".item h2{margin:0 0 8px;font-size:18px;}"
        ".meta{color:#666;font-size:12px;margin-bottom:12px;}"
        ".content img{max-width:100%;border-radius:10px;margin:8px 0;}"
        ".content video{max-width:100%;margin:8px 0;}"
        ".content iframe{width:100%;min-height:315px;border:0;border-radius:10px;margin:8px 0;}"
        "</style></head><body>"
        f"<h1><a href=\"{_xml_escape(link)}\">{_xml_escape(title)}</a></h1>"
        + "".join(cards)
        + "</body></html>"
    )


def _parse_media_json(raw: Optional[str]) -> List[Dict[str, Any]]:
    if not raw:
        return []
    try:
        data = json.loads(raw)
        return data if isinstance(data, list) else []
    except Exception:
        return []


def parse_datetime(dt_str: Optional[str]) -> Optional[datetime]:
    """ë‹¤ì–‘í•œ í˜•ì‹ì˜ ë‚ ì§œ ë¬¸ìì—´ì„ datetimeìœ¼ë¡œ íŒŒì‹±."""
    if not dt_str:
        return None
    try:
        return date_parser.parse(dt_str)
    except Exception:
        return None


def make_aware(dt: datetime) -> datetime:
    """datetimeì„ timezone-awareë¡œ ë³€í™˜ (UTC ê¸°ì¤€)."""
    if dt.tzinfo is None:
        return dt.replace(tzinfo=timezone.utc)
    return dt


def compute_cutoff_utc(
    since_days: Optional[int] = None,
    since_date: Optional[str] = None,
) -> Optional[datetime]:
    """since_days/since_dateë¡œ cutoff(UTC, timezone-aware) ê³„ì‚°.
    
    - since_dateê°€ ìˆìœ¼ë©´ since_daysë³´ë‹¤ ìš°ì„ 
    - filter_posts_by_dateì™€ ë™ì¼í•œ ì˜ë¯¸ë¥¼ ìœ ì§€
    """
    if since_date:
        parsed = parse_datetime(since_date)
        if not parsed:
            return None
        if parsed.tzinfo is None:
            parsed = parsed.replace(tzinfo=KST)
        return parsed.astimezone(UTC)
    if since_days:
        return datetime.now(UTC) - timedelta(days=since_days)
    return None


def filter_posts_by_date(
    posts: List[Dict[str, Any]],
    since_days: Optional[int] = None,
    since_date: Optional[str] = None,
) -> List[Dict[str, Any]]:
    """ê²Œì‹œë¬¼ì„ ë‚ ì§œ ê¸°ì¤€ìœ¼ë¡œ í•„í„°ë§ (í•œêµ­ ì‹œê°„ KST ê¸°ì¤€).
    
    Args:
        posts: ê²Œì‹œë¬¼ ë¦¬ìŠ¤íŠ¸
        since_days: ìµœê·¼ Nì¼ ì´ë‚´ ê²Œì‹œë¬¼ë§Œ (ì˜ˆ: 7 = ì¼ì£¼ì¼)
        since_date: íŠ¹ì • ë‚ ì§œ ì´í›„ ê²Œì‹œë¬¼ë§Œ (ISO í˜•ì‹, ì˜ˆ: "2024-01-01")
    """
    if not since_days and not since_date:
        return posts
    
    # ê¸°ì¤€ ë‚ ì§œ ê³„ì‚° (í•œêµ­ ì‹œê°„ KST ê¸°ì¤€)
    now_kst = datetime.now(KST)
    
    if since_days:
        # í˜„ì¬ ì‹œê°„ ê¸°ì¤€ìœ¼ë¡œ N*24ì‹œê°„ ì „ë¶€í„° (ë” ì§ê´€ì ì¸ ë°©ì‹)
        cutoff = datetime.now(UTC) - timedelta(days=since_days)
        print(f"[filter] í˜„ì¬ KST: {now_kst.strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"[filter] cutoff UTC: {cutoff.strftime('%Y-%m-%d %H:%M:%S')} (since_days={since_days})")
    elif since_date:
        parsed = parse_datetime(since_date)
        if not parsed:
            return posts  # íŒŒì‹± ì‹¤íŒ¨ ì‹œ í•„í„°ë§ ì—†ì´ ë°˜í™˜
        # ì…ë ¥ëœ ë‚ ì§œë¥¼ í•œêµ­ ì‹œê°„ìœ¼ë¡œ í•´ì„
        if parsed.tzinfo is None:
            parsed = parsed.replace(tzinfo=KST)
        cutoff = parsed.astimezone(UTC)
    else:
        return posts
    
    filtered = []
    for i, post in enumerate(posts):
        created_at = parse_datetime(post.get("created_at"))
        url = post.get("url", "")
        post_id = url.split("/post/")[-1] if "/post/" in url else f"post_{i}"
        
        if created_at is None:
            # ë‚ ì§œ ì •ë³´ ì—†ìœ¼ë©´ í¬í•¨ (ë³´ìˆ˜ì  ì²˜ë¦¬)
            filtered.append(post)
            print(f"[filter] {post_id}: created_at=None â†’ í¬í•¨")
        else:
            # UTCë¡œ ë³€í™˜í•˜ì—¬ ë¹„êµ
            created_at_aware = make_aware(created_at)
            if created_at_aware >= cutoff:
                filtered.append(post)
                print(f"[filter] {post_id}: {created_at_aware} >= {cutoff} â†’ âœ… í¬í•¨")
            else:
                print(f"[filter] {post_id}: {created_at_aware} < {cutoff} â†’ âŒ ì œì™¸")
    
    print(f"[filter] ê²°ê³¼: {len(posts)}ê°œ ì¤‘ {len(filtered)}ê°œ í†µê³¼")
    return filtered


class ScrapeRequest(BaseModel):
    """ìŠ¤í¬ë˜í•‘ ìš”ì²­ ëª¨ë¸ (ì™¸ë¶€ ì‚¬ìš©ììš©)"""
    username: str = Field(..., description="Threads ì‚¬ìš©ìëª… (ì˜ˆ: 'zuck', '@' ì—†ì´ ì…ë ¥)", json_schema_extra={"example": "zuck"})
    max_posts: Optional[int] = Field(None, description="ìµœëŒ€ ìˆ˜ì§‘í•  ê²Œì‹œë¬¼ ìˆ˜ (ë¯¸ì§€ì •ì‹œ ì „ì²´)", json_schema_extra={"example": 10})
    since_days: Optional[int] = Field(None, description="ìµœê·¼ Nì¼ ì´ë‚´ ê²Œì‹œë¬¼ë§Œ (ì˜ˆ: 7=ì¼ì£¼ì¼, 30=í•œë‹¬)", ge=1, le=365, json_schema_extra={"example": 7})
    since_date: Optional[str] = Field(None, description="íŠ¹ì • ë‚ ì§œ ì´í›„ ê²Œì‹œë¬¼ë§Œ (ISO í˜•ì‹, ì˜ˆ: '2024-12-01')", json_schema_extra={"example": "2024-12-01"})
    
    model_config = {
        "json_schema_extra": {
            "examples": [
                {
                    "username": "zuck",
                    "max_posts": 10,
                    "since_days": 7
                }
            ]
        }
    }


class PostResponse(BaseModel):
    """ê²Œì‹œë¬¼ ì‘ë‹µ ëª¨ë¸"""
    text: str
    created_at: Optional[str]
    url: str
    media: List[Dict[str, str]] = []


class ScrapeResponse(BaseModel):
    """ìŠ¤í¬ë˜í•‘ ì‘ë‹µ ëª¨ë¸"""
    username: str
    total_posts: int
    filtered_posts: int
    posts: List[PostResponse]
    scraped_at: str
    filter_applied: Optional[str] = None


class BatchScrapeRequest(BaseModel):
    """ë°°ì¹˜ ìŠ¤í¬ë˜í•‘ ìš”ì²­ ëª¨ë¸ (ë ˆê±°ì‹œ í˜¸í™˜ìš©)"""
    usernames: List[str] = Field(..., description="Threads ì‚¬ìš©ìëª… ë¦¬ìŠ¤íŠ¸ (ì˜ˆ: ['zuck', 'meta'])", min_length=1, max_length=50)
    max_posts: Optional[int] = Field(None, description="ê° ê³„ì •ë‹¹ ìµœëŒ€ ìˆ˜ì§‘í•  ê²Œì‹œë¬¼ ìˆ˜ (Noneì´ë©´ ì œí•œ ì—†ìŒ)")
    max_scroll_rounds: int = Field(50, description="ê° ê³„ì •ë‹¹ ìµœëŒ€ ìŠ¤í¬ë¡¤ ë¼ìš´ë“œ ìˆ˜", ge=1, le=200)
    since_days: Optional[int] = Field(None, description="ìµœê·¼ Nì¼ ì´ë‚´ ê²Œì‹œë¬¼ë§Œ", ge=1, le=365)
    since_date: Optional[str] = Field(None, description="íŠ¹ì • ë‚ ì§œ ì´í›„ ê²Œì‹œë¬¼ë§Œ (ISO í˜•ì‹)")
    
    model_config = {
        "json_schema_extra": {
            "examples": [
                {
                    "usernames": ["zuck", "meta", "instagram"],
                    "max_posts": 10,
                    "since_days": 7
                }
            ]
        }
    }


class BatchScrapeRequestV2(BaseModel):
    """ë°°ì¹˜ ìŠ¤í¬ë˜í•‘ ìš”ì²­ ëª¨ë¸"""
    usernames: List[str] = Field(..., description="Threads ì‚¬ìš©ìëª… ë¦¬ìŠ¤íŠ¸ (ì˜ˆ: ['zuck', 'meta'])", min_length=1, max_length=50)
    since_days: int = Field(1, description="ìµœê·¼ Nì¼ ì´ë‚´ ê²Œì‹œë¬¼ë§Œ (ê¸°ë³¸: 1ì¼)", ge=1, le=365)
    since_date: Optional[str] = Field(None, description="íŠ¹ì • ë‚ ì§œ ì´í›„ ê²Œì‹œë¬¼ë§Œ (ISO í˜•ì‹, since_daysë³´ë‹¤ ìš°ì„ )")
    max_per_account: int = Field(10, description="ê³„ì •ë³„ ìµœëŒ€ ìŠ¤í¬ë© ê°¯ìˆ˜ (ê¸°ë³¸: 10)", ge=1, le=100)
    max_total: int = Field(300, description="ì „ì²´ ìµœëŒ€ ìŠ¤í¬ë© ê°¯ìˆ˜ - ë„ë‹¬ ì‹œ ì¤‘ì§€ (ê¸°ë³¸: 300)", ge=1, le=1000)
    include_replies: bool = Field(True, description="ë‹µê¸€ í¬í•¨ ì—¬ë¶€ (ê¸°ë³¸: True)")
    max_reply_depth: int = Field(1, description="ë‹µê¸€ ìˆ˜ì§‘ ì‹œ ìµœëŒ€ ê¹Šì´ (ê¸°ë³¸: 1)", ge=1, le=10)
    
    model_config = {
        "json_schema_extra": {
            "examples": [
                {
                    "usernames": ["zuck", "meta", "instagram"],
                    "max_per_account": 10,
                    "max_total": 300
                }
            ]
        }
    }


class V2BatchScrapeRequest(BaseModel):
    """ìƒˆë¡œìš´ ì‘ë‹µ í˜•ì‹ìš© ë°°ì¹˜ ìŠ¤í¬ë˜í•‘ ìš”ì²­ ëª¨ë¸"""
    usernames: List[str] = Field(..., description="Threads ì‚¬ìš©ìëª… ë¦¬ìŠ¤íŠ¸", min_length=1, max_length=50)
    since_days: Optional[int] = Field(1, description="ìµœê·¼ Nì¼ ì´ë‚´ ê²Œì‹œë¬¼ë§Œ (ê¸°ë³¸: 1ì¼)", ge=1, le=365)
    since_date: Optional[str] = Field(None, description="íŠ¹ì • ë‚ ì§œ ì´í›„ ê²Œì‹œë¬¼ë§Œ (since_daysë³´ë‹¤ ìš°ì„ )")
    include_replies: bool = Field(True, description="ë‹µê¸€ í¬í•¨ ì—¬ë¶€ (ê¸°ë³¸: True)")
    max_reply_depth: int = Field(1, description="ë‹µê¸€ ìˆ˜ì§‘ ì‹œ ìµœëŒ€ ê¹Šì´ (ê¸°ë³¸: 1)", ge=1, le=10)
    max_per_account: int = Field(10, description="ê³„ì •ë³„ ìµœëŒ€ ìŠ¤í¬ë© ê°¯ìˆ˜ (ê¸°ë³¸: 10)", ge=1, le=100)
    max_total: int = Field(300, description="ì „ì²´ ìµœëŒ€ ìŠ¤í¬ë© ê°¯ìˆ˜ (ê¸°ë³¸: 300)", ge=1, le=1000)
    response_style: Literal["threaded", "flat"] = Field(
        "threaded",
        description="ì‘ë‹µ ìŠ¤íƒ€ì¼: threaded(ìŠ¤ë ˆë“œ êµ¬ì¡°) | flat(í”Œë« êµ¬ì¡°)"
    )
    
    model_config = {
        "json_schema_extra": {
            "examples": [
                {
                    "usernames": ["zuck", "meta", "instagram"],
                    "since_days": 7,
                    "max_per_account": 10,
                    "max_total": 300,
                    "response_style": "threaded",
                }
            ]
        }
    }


class AdminAccountCreate(BaseModel):
    username: str = Field(..., description="Threads username")
    display_name: Optional[str] = None
    profile_url: Optional[str] = None
    include_replies: Optional[bool] = True
    max_reply_depth: Optional[int] = Field(1, ge=1, le=10)


class AdminAccountUpdate(BaseModel):
    is_active: Optional[bool] = None
    display_name: Optional[str] = None
    profile_url: Optional[str] = None
    include_replies: Optional[bool] = None
    max_reply_depth: Optional[int] = Field(None, ge=1, le=10)


class AdminScrapeRequest(BaseModel):
    usernames: Optional[List[str]] = None
    all_accounts: bool = False
    max_total_posts: Optional[int] = Field(None, ge=1, le=1000)


class AdminScheduleUpdate(BaseModel):
    is_active: Optional[bool] = None
    interval_minutes: Optional[int] = Field(None, ge=5, le=1440)
    start_time: Optional[str] = Field(None, description="HH:MM 24h (KST)")


class AdminCacheRefreshRequest(BaseModel):
    usernames: Optional[List[str]] = None
    all_accounts: bool = False


class AdminTokenCreate(BaseModel):
    scope: str = Field(..., description="username or 'global'")
    is_active: bool = True


class AdminRateLimitUpdate(BaseModel):
    window_seconds: Optional[int] = Field(None, ge=60, le=3600)
    max_requests: Optional[int] = Field(None, ge=1, le=10000)


class AdminCachePolicyUpdate(BaseModel):
    enabled: Optional[bool] = None
    ttl_seconds: Optional[int] = Field(None, ge=30, le=86400)


class CurationConfigUpdate(BaseModel):
    candidate_generation_enabled: Optional[bool] = None
    auto_publish_enabled: Optional[bool] = None
    rss_public_enabled: Optional[bool] = None
    target_daily_spend_usd: Optional[float] = Field(None, ge=0.0)
    daily_run_time_kst: Optional[str] = None
    publish_delay_minutes: Optional[int] = Field(None, ge=0, le=1440)
    feed_size: Optional[int] = Field(None, ge=1, le=50)
    window_hours: Optional[int] = Field(None, ge=1, le=168)
    candidate_pool_size: Optional[int] = Field(None, ge=1, le=200)
    w_impact: Optional[float] = Field(None, ge=0.0, le=1.0)
    w_novelty: Optional[float] = Field(None, ge=0.0, le=1.0)
    w_utility: Optional[float] = Field(None, ge=0.0, le=1.0)
    w_accessibility: Optional[float] = Field(None, ge=0.0, le=1.0)
    recommend_threshold: Optional[int] = Field(None, ge=0, le=100)
    min_utility: Optional[int] = Field(None, ge=0, le=100)
    min_accessibility: Optional[int] = Field(None, ge=0, le=100)
    pick_easy: Optional[int] = Field(None, ge=0, le=100)
    pick_deep: Optional[int] = Field(None, ge=0, le=100)
    pick_action: Optional[int] = Field(None, ge=0, le=100)
    openrouter_model: Optional[str] = None
    openrouter_api_key: Optional[str] = None
    input_cost_per_1m: Optional[float] = Field(None, ge=0.0)
    output_cost_per_1m: Optional[float] = Field(None, ge=0.0)
    llm_enabled: Optional[bool] = None


class CurationRunRequest(BaseModel):
    force: bool = True


class CurationCandidateUpdate(BaseModel):
    action: Optional[Literal["approve", "reject", "pending"]] = None
    edited_title: Optional[str] = None
    edited_summary: Optional[str] = None
    edited_image_url: Optional[str] = None
    selected_for_publish: Optional[bool] = None
    review_note: Optional[str] = None


class CurationReorderRequest(BaseModel):
    run_id: int
    ordered_ids: List[int] = Field(..., min_length=1)


class CurationApproveAllRequest(BaseModel):
    run_id: int


class CurationPublishRequest(BaseModel):
    run_id: Optional[int] = None
    publish_mode: str = "manual"


class CurationCacheRefreshRequest(BaseModel):
    feed_type: Literal["all", "item", "digest"] = "all"
    limit: int = Field(10, ge=1, le=100)


class CurationStage1Request(BaseModel):
    hours: int = Field(24, ge=1, le=168)
    force: bool = True


class CurationStageRunRequest(BaseModel):
    run_id: int


class CurationStage3Request(BaseModel):
    run_id: int
    candidate_id: Optional[int] = None
    force: bool = False


class BatchScrapeItem(BaseModel):
    """ë°°ì¹˜ ìŠ¤í¬ë˜í•‘ ê²°ê³¼ í•­ëª©"""
    username: str
    success: bool
    total_posts: int
    filtered_posts: int
    posts: List[PostResponse]
    error: Optional[str] = None
    scraped_at: str
    filter_applied: Optional[str] = None


class BatchScrapeResponse(BaseModel):
    """ë°°ì¹˜ ìŠ¤í¬ë˜í•‘ ì‘ë‹µ ëª¨ë¸"""
    total_accounts: int
    successful_accounts: int
    failed_accounts: int
    results: List[BatchScrapeItem]
    completed_at: str


class UserSearchResult(BaseModel):
    """ì‚¬ìš©ì ê²€ìƒ‰ ê²°ê³¼ í•­ëª©"""
    username: str
    display_name: str
    profile_url: str


class UserSearchResponse(BaseModel):
    """ì‚¬ìš©ì ê²€ìƒ‰ ì‘ë‹µ ëª¨ë¸"""
    query: str
    total_results: int
    users: List[UserSearchResult]


# ============ ë‹µê¸€ í¬í•¨ ìŠ¤í¬ë˜í•‘ ëª¨ë¸ ============

class ReplyPost(BaseModel):
    """ë‹µê¸€ ê²Œì‹œë¬¼ ëª¨ë¸ (ì¬ê·€ êµ¬ì¡°)"""
    post_id: Optional[str] = None
    text: str
    created_at: Optional[str] = None
    url: str
    author: Optional[str] = None
    media: List[Dict[str, str]] = []
    replies: List["ReplyPost"] = []

# Pydantic v2 forward reference í•´ê²°
ReplyPost.model_rebuild()


class ThreadWithRepliesResponse(BaseModel):
    """ë‹µê¸€ í¬í•¨ ìŠ¤ë ˆë“œ ì‘ë‹µ ëª¨ë¸"""
    post_id: Optional[str] = None
    text: str
    created_at: Optional[str] = None
    url: str
    author: Optional[str] = None
    media: List[Dict[str, str]] = []
    replies: List[ReplyPost] = []
    total_replies_count: int = 0
    scraped_at: str


class ScrapeThreadRequest(BaseModel):
    """ê°œë³„ ìŠ¤ë ˆë“œ ìŠ¤í¬ë˜í•‘ ìš”ì²­ ëª¨ë¸"""
    post_url: str = Field(..., description="ê²Œì‹œë¬¼ URL (ì˜ˆ: 'https://www.threads.com/@user/post/ABC123')")
    max_depth: int = Field(1, description="ìµœëŒ€ ë‹µê¸€ ê¹Šì´ (ê¸°ë³¸: ë£¨íŠ¸ + ì§ì ‘ reply)", ge=1, le=10)
    max_replies_per_level: int = Field(100, description="ê° ë ˆë²¨ë‹¹ ìµœëŒ€ ë‹µê¸€ ìˆ˜", ge=1, le=500)
    
    model_config = {
        "json_schema_extra": {
            "examples": [
                {
                    "post_url": "https://www.threads.com/@zuck/post/ABC123",
                    "max_depth": 5,
                    "max_replies_per_level": 100
                }
            ]
        }
    }


class ScrapeWithRepliesRequest(BaseModel):
    """ë‹µê¸€ í¬í•¨ í”„ë¡œí•„ ìŠ¤í¬ë˜í•‘ ìš”ì²­ ëª¨ë¸"""
    username: str = Field(..., description="Threads ì‚¬ìš©ìëª…")
    max_posts: Optional[int] = Field(None, description="ìµœëŒ€ ìˆ˜ì§‘í•  ê²Œì‹œë¬¼ ìˆ˜")
    include_replies: bool = Field(True, description="ë‹µê¸€ í¬í•¨ ì—¬ë¶€")
    max_reply_depth: int = Field(1, description="ìµœëŒ€ ë‹µê¸€ ê¹Šì´ (ê¸°ë³¸: ë£¨íŠ¸ + ì§ì ‘ reply)", ge=1, le=10)
    since_days: Optional[int] = Field(None, description="ìµœê·¼ Nì¼ ì´ë‚´ ê²Œì‹œë¬¼ë§Œ", ge=1, le=365)
    since_date: Optional[str] = Field(None, description="íŠ¹ì • ë‚ ì§œ ì´í›„ ê²Œì‹œë¬¼ë§Œ (ISO í˜•ì‹)")
    
    model_config = {
        "json_schema_extra": {
            "examples": [
                {
                    "username": "zuck",
                    "max_posts": 5,
                    "include_replies": True,
                    "max_reply_depth": 3
                }
            ]
        }
    }


class ScrapeWithRepliesResponse(BaseModel):
    """ë‹µê¸€ í¬í•¨ ìŠ¤í¬ë˜í•‘ ì‘ë‹µ ëª¨ë¸"""
    username: str
    total_posts: int
    posts: List[ThreadWithRepliesResponse]
    scraped_at: str
    filter_applied: Optional[str] = None


class V2ScrapeRequest(BaseModel):
    """ìƒˆë¡œìš´ ì‘ë‹µ í˜•ì‹ìš© ìŠ¤í¬ë˜í•‘ ìš”ì²­ ëª¨ë¸"""
    username: str = Field(..., description="Threads ì‚¬ìš©ìëª… (ì˜ˆ: 'zuck', '@' ì—†ì´ ì…ë ¥)")
    since_days: Optional[int] = Field(1, description="ìµœê·¼ Nì¼ ì´ë‚´ ê²Œì‹œë¬¼ë§Œ (ê¸°ë³¸: 1ì¼)", ge=1, le=365)
    since_date: Optional[str] = Field(None, description="íŠ¹ì • ë‚ ì§œ ì´í›„ ê²Œì‹œë¬¼ë§Œ (ISO í˜•ì‹, since_daysë³´ë‹¤ ìš°ì„ )")
    include_replies: bool = Field(True, description="ë‹µê¸€ í¬í•¨ ì—¬ë¶€ (ê¸°ë³¸: True)")
    max_reply_depth: int = Field(1, description="ë‹µê¸€ ìˆ˜ì§‘ ì‹œ ìµœëŒ€ ê¹Šì´ (ê¸°ë³¸: ë£¨íŠ¸ + ì§ì ‘ reply)", ge=1, le=10)
    max_total_posts: int = Field(100, description="ì „ì²´ ì¶œë ¥ ìµœëŒ€ ê²Œì‹œë¬¼ ìˆ˜ (ëª¨ë“  root + ëª¨ë“  ë‹µê¸€ í•©ê³„, ê¸°ë³¸: 100)", ge=1, le=1000)
    response_style: Literal["threaded", "flat"] = Field(
        "threaded",
        description="ì‘ë‹µ ìŠ¤íƒ€ì¼: threaded(ìŠ¤ë ˆë“œ êµ¬ì¡°) | flat(í”Œë« êµ¬ì¡°)"
    )

    model_config = {
        "json_schema_extra": {
            "examples": [
                {
                    "username": "zuck",
                    "since_days": 7,
                    "include_replies": True,
                    "max_reply_depth": 2,
                    "max_total_posts": 100,
                    "response_style": "threaded",
                }
            ]
        }
    }


def build_v2_data_from_result(
    result: Dict[str, Any],
    response_style: Literal["threaded", "flat"],
    include_replies: bool,
) -> Dict[str, Any]:
    def normalize_post(post: Dict[str, Any]) -> Dict[str, Any]:
        return {
            "id": post.get("post_id"),
            "url": post.get("url"),
            "text": post.get("text", ""),
            "created_at": post.get("created_at"),
            "author": post.get("author"),
            "media": post.get("media", []),
        }
    
    def normalize_simple_post(post: Dict[str, Any]) -> Dict[str, Any]:
        return {
            "id": None,
            "url": post.get("url"),
            "text": post.get("text", ""),
            "created_at": post.get("created_at"),
            "author": None,
            "media": post.get("media", []),
        }
    
    def flatten_replies(replies: List[Dict[str, Any]], parent_id: Optional[str]) -> List[Dict[str, Any]]:
        flat = []
        for r in replies:
            flat.append({
                "post": {
                    "id": r.get("post_id"),
                    "url": r.get("url"),
                    "text": r.get("text", ""),
                    "created_at": r.get("created_at"),
                    "author": r.get("author"),
                    "media": r.get("media", []),
                },
                "parent_id": parent_id,
            })
            child_replies = r.get("replies", [])
            if child_replies:
                flat.extend(flatten_replies(child_replies, r.get("post_id")))
        return flat
    
    if include_replies:
        threads = []
        posts_flat = []
        for post in result["posts"]:
            root = normalize_post(post)
            replies = post.get("replies", [])
            
            if response_style == "flat":
                posts_flat.append({
                    "type": "root",
                    "thread_id": root["id"],
                    "post": root,
                    "parent_id": None,
                })
                for r in flatten_replies(replies, root["id"]):
                    posts_flat.append({
                        "type": "reply",
                        "thread_id": root["id"],
                        "post": r["post"],
                        "parent_id": r["parent_id"],
                    })
            else:
                threads.append({
                    "thread_id": root["id"],
                    "post": root,
                    "replies": replies,
                    "counts": {
                        "replies": post.get("total_replies_count", 0),
                        "total": 1 + post.get("total_replies_count", 0),
                    },
                })
        
        if response_style == "flat":
            return {"style": "flat", "items": posts_flat}
        return {"style": "threaded", "items": threads}
    
    # ë‹µê¸€ ë¯¸í¬í•¨
    threads = []
    posts_flat = []
    for post in result["posts"]:
        root = normalize_simple_post(post)
        if response_style == "flat":
            posts_flat.append({
                "type": "root",
                "thread_id": None,
                "post": root,
                "parent_id": None,
            })
        else:
            threads.append({
                "thread_id": None,
                "post": root,
                "replies": [],
                "counts": {"replies": 0, "total": 1},
            })
    
    if response_style == "flat":
        return {"style": "flat", "items": posts_flat}
    return {"style": "threaded", "items": threads}


@app.on_event("startup")
async def on_startup():
    if os.environ.get("ENABLE_INTERNAL_SCHEDULER") == "1":
        asyncio.create_task(_scheduler_loop())


@app.get("/v2/rss")
def rss_feed(
    request: Request,
    username: str = Query(..., description="Threads username"),
    token: str = Query(..., description="Access token"),
    limit: int = Query(50, ge=1, le=500),
):
    username = username.lstrip("@").strip()
    if not username:
        raise HTTPException(status_code=400, detail="username is required")
    conn = _get_db_conn()
    try:
        tok = conn.execute(
            "SELECT id, token, scope, is_active FROM tokens WHERE token = ?",
            (token,),
        ).fetchone()
        if not tok or not tok[2]:
            _log_invalid_token(conn, token, username, request, 401)
            raise HTTPException(status_code=401, detail="invalid token")
        token_id = tok[0]
        scope = tok[2]
        if scope not in ("global", "*", username):
            _log_token_request(conn, token_id, username, request, 403)
            raise HTTPException(status_code=403, detail="token scope mismatch")

        # rate limit
        rl = _get_rate_limit(conn)
        window = int(datetime.now(timezone.utc).timestamp()) // rl["window_seconds"]
        row = conn.execute(
            "SELECT count FROM rss_token_counters WHERE token_id = ? AND window_start = ?",
            (token_id, window),
        ).fetchone()
        if row and row[0] >= rl["max_requests"]:
            _log_token_request(conn, token_id, username, request, 429)
            raise HTTPException(status_code=429, detail="rate limit exceeded")
        if row:
            conn.execute(
                "UPDATE rss_token_counters SET count = count + 1 WHERE token_id = ? AND window_start = ?",
                (token_id, window),
            )
        else:
            conn.execute(
                "INSERT INTO rss_token_counters (token_id, window_start, count) VALUES (?, ?, 1)",
                (token_id, window),
            )
        conn.commit()
        
        src = conn.execute(
            "SELECT id, username FROM feed_sources WHERE username = ?",
            (username,),
        ).fetchone()
        if not src:
            raise HTTPException(status_code=404, detail="account not found")
        
        roots = conn.execute(
            "SELECT post_id, url, text, media_json, created_at FROM posts "
            "WHERE source_id = ? AND is_reply = 0 ORDER BY created_at DESC LIMIT ?",
            (src[0], limit),
        ).fetchall()
        
        channel_title = f"{username} Threads Feed"
        channel_link = f"https://www.threads.com/@{username}"
        channel_desc = f"Threads posts scraped for @{username}"
        
        items = []
        preview_items = []
        for r in roots:
            post_id, url, text, media_json, created_at = r
            root_media = _parse_media_json(media_json)
            replies = conn.execute(
                "SELECT text, media_json FROM posts WHERE source_id = ? AND is_reply = 1 AND parent_post_id = ? "
                "ORDER BY created_at ASC",
                (src[0], post_id),
            ).fetchall()
            media_urls = []
            root_media_urls = []
            if root_media:
                root_media_urls = [m.get("url") for m in root_media if m.get("url")]
                media_urls.extend(root_media_urls)
            for rr in replies:
                rep_media = _parse_media_json(rr[1])
                if rep_media:
                    media_urls.extend([m.get("url") for m in rep_media if m.get("url")])
            # dedupe media
            seen_media = set()
            media_urls = [u for u in media_urls if u and not (u in seen_media or seen_media.add(u))]
            desc_html = _build_description_html(text or "", replies)
            created_dt = None
            try:
                created_dt = date_parser.parse(created_at) if created_at else None
            except Exception:
                created_dt = None
            if created_dt and created_dt.tzinfo is None:
                created_dt = created_dt.replace(tzinfo=timezone.utc)
            pub_date = format_datetime(created_dt) if created_dt else format_datetime(datetime.now(timezone.utc))
            title_source = (text or "").strip()
            if not title_source:
                for rr in replies:
                    rep_text = (rr[0] or "").strip()
                    if rep_text:
                        title_source = rep_text
                        break
            title = (title_source or "(no title)").split("\n")[0][:80]
            enclosures = _build_enclosures(media_urls)
            media_contents = _build_media_contents(media_urls)
            youtube_embeds = _collect_youtube_embeds(text or "", replies)
            media_players = _build_media_players(youtube_embeds)
            content_html = _build_content_html(text or "", replies, root_media_urls, media_urls, youtube_embeds)
            items.append(
                f"<item>"
                f"<title>{_xml_escape(title)}</title>"
                f"<link>{_xml_escape(url)}</link>"
                f"<guid>{_xml_escape(post_id or url)}</guid>"
                f"<pubDate>{pub_date}</pubDate>"
                f"<description>{desc_html}</description>"
                f"<content:encoded><![CDATA[{content_html}]]></content:encoded>"
                f"{enclosures}"
                f"{media_contents}"
                f"{media_players}"
                f"</item>"
            )
            preview_items.append(
                {
                    "title": title,
                    "link": url,
                    "pubDate": pub_date,
                    "content_html": content_html,
                }
            )
        
        accept_header = request.headers.get("accept", "")
        if "text/html" in accept_header.lower():
            html = _build_html_preview(channel_title, channel_link, preview_items)
            return Response(content=html, media_type="text/html; charset=utf-8")

        cache_policy = _get_cache_policy(conn)
        cache_row = None
        if cache_policy["enabled"]:
            cache_row = conn.execute(
                "SELECT etag, last_modified, xml, updated_at FROM rss_feed_cache WHERE username = ? AND limit_count = ?",
                (username, limit),
            ).fetchone()
        if cache_row:
            cached_etag, cached_last, cached_xml, cached_updated = cache_row
            cache_ok = True
            if cache_policy["ttl_seconds"] > 0 and cached_updated:
                try:
                    updated_dt = date_parser.parse(cached_updated)
                    if updated_dt.tzinfo is None:
                        updated_dt = updated_dt.replace(tzinfo=timezone.utc)
                    cache_ok = datetime.now(timezone.utc) - updated_dt <= timedelta(seconds=cache_policy["ttl_seconds"])
                except Exception:
                    cache_ok = False
            if cache_ok:
                inm = request.headers.get("if-none-match")
                ims = request.headers.get("if-modified-since")
                if inm and inm == cached_etag:
                    _log_token_request(conn, token_id, username, request, 304)
                    return PlainTextResponse("", status_code=304)
                if ims and cached_last and ims == cached_last:
                    _log_token_request(conn, token_id, username, request, 304)
                    return PlainTextResponse("", status_code=304)
                headers = {"ETag": cached_etag}
                if cached_last:
                    headers["Last-Modified"] = cached_last
                _log_token_request(conn, token_id, username, request, 200)
                return Response(
                    content=cached_xml.encode("utf-8"),
                    media_type="application/rss+xml; charset=utf-8",
                    headers=headers,
                )

        xml = (
            "<?xml version=\"1.0\" encoding=\"UTF-8\"?>"
            "<?xml-stylesheet type=\"text/xsl\" href=\"/rss.xsl?v=2\"?>"
            "<rss version=\"2.0\" xmlns:content=\"http://purl.org/rss/1.0/modules/content/\" xmlns:media=\"http://search.yahoo.com/mrss/\">"
            "<channel>"
            f"<title>{_xml_escape(channel_title)}</title>"
            f"<link>{_xml_escape(channel_link)}</link>"
            f"<description>{_xml_escape(channel_desc)}</description>"
            + "".join(items)
            + "</channel></rss>"
        )
        etag = hashlib.sha256(xml.encode("utf-8")).hexdigest()
        last_modified = None
        if roots:
            try:
                last_modified_dt = date_parser.parse(roots[0][4]) if roots[0][4] else None
                if last_modified_dt and last_modified_dt.tzinfo is None:
                    last_modified_dt = last_modified_dt.replace(tzinfo=timezone.utc)
                if last_modified_dt:
                    last_modified = format_datetime(last_modified_dt)
            except Exception:
                last_modified = None
        inm = request.headers.get("if-none-match")
        ims = request.headers.get("if-modified-since")
        if inm and inm == etag:
            _log_token_request(conn, token_id, username, request, 304)
            return PlainTextResponse("", status_code=304)
        if ims and last_modified and ims == last_modified:
            _log_token_request(conn, token_id, username, request, 304)
            return PlainTextResponse("", status_code=304)

        headers = {"ETag": etag}
        if last_modified:
            headers["Last-Modified"] = last_modified
        _log_token_request(conn, token_id, username, request, 200)
        if cache_policy["enabled"]:
            try:
                now = datetime.now(timezone.utc).isoformat()
                conn.execute(
                    "INSERT OR REPLACE INTO rss_feed_cache (username, limit_count, etag, last_modified, xml, updated_at) "
                    "VALUES (?, ?, ?, ?, ?, ?)",
                    (username, limit, etag, last_modified, xml, now),
                )
                conn.commit()
            except Exception:
                pass
        return Response(
            content=xml.encode("utf-8"),
            media_type="application/rss+xml; charset=utf-8",
            headers=headers,
        )
    finally:
        conn.close()


def _authorize_curated_rss_request(
    conn: sqlite3.Connection,
    request: Request,
    token: str,
) -> int:
    tok = conn.execute(
        "SELECT id, scope, is_active FROM tokens WHERE token = ?",
        (token,),
    ).fetchone()
    if not tok or not tok[2]:
        _log_invalid_token(conn, token, "curated", request, 401)
        raise HTTPException(status_code=401, detail="invalid token")
    token_id = int(tok[0])
    scope = str(tok[1] or "")
    if scope not in ("global", "*", "curated"):
        _log_token_request(conn, token_id, "curated", request, 403)
        raise HTTPException(status_code=403, detail="token scope mismatch")

    rl = _get_rate_limit(conn)
    window = int(datetime.now(timezone.utc).timestamp()) // rl["window_seconds"]
    row = conn.execute(
        "SELECT count FROM rss_token_counters WHERE token_id = ? AND window_start = ?",
        (token_id, window),
    ).fetchone()
    if row and row[0] >= rl["max_requests"]:
        _log_token_request(conn, token_id, "curated", request, 429)
        raise HTTPException(status_code=429, detail="rate limit exceeded")
    if row:
        conn.execute(
            "UPDATE rss_token_counters SET count = count + 1 WHERE token_id = ? AND window_start = ?",
            (token_id, window),
        )
    else:
        conn.execute(
            "INSERT INTO rss_token_counters (token_id, window_start, count) VALUES (?, ?, 1)",
            (token_id, window),
        )
    conn.commit()
    return token_id


def _serve_curated_feed(
    request: Request,
    token: str,
    feed_type: str,
    limit: int,
) -> Response:
    conn = _get_db_conn()
    try:
        token_id = _authorize_curated_rss_request(conn, request, token)
        cfg = curation_get_config(conn)
        if not bool(cfg.get("rss_public_enabled", 1)):
            _log_token_request(conn, token_id, "curated", request, 403)
            raise HTTPException(status_code=403, detail="curated rss is disabled")

        cached = curation_get_cached_feed(conn, feed_type, limit)
        etag = cached.get("etag")
        last_modified = cached.get("last_modified")
        xml = cached.get("xml") or ""

        inm = request.headers.get("if-none-match")
        ims = request.headers.get("if-modified-since")
        if inm and etag and inm == etag:
            _log_token_request(conn, token_id, "curated", request, 304)
            return PlainTextResponse("", status_code=304)
        if ims and last_modified and ims == last_modified:
            _log_token_request(conn, token_id, "curated", request, 304)
            return PlainTextResponse("", status_code=304)

        headers = {}
        if etag:
            headers["ETag"] = etag
        if last_modified:
            headers["Last-Modified"] = last_modified
        _log_token_request(conn, token_id, "curated", request, 200)
        return Response(
            content=xml.encode("utf-8"),
            media_type="application/rss+xml; charset=utf-8",
            headers=headers,
        )
    finally:
        conn.close()


@app.get("/v2/rss/curated")
def rss_curated_feed(
    request: Request,
    token: str = Query(..., description="Curated RSS token (scope=curated/global)"),
    limit: int = Query(10, ge=1, le=100),
):
    return _serve_curated_feed(request, token, CURATED_ITEM_FEED, limit)


@app.get("/v2/rss/curated/digest")
def rss_curated_digest(
    request: Request,
    token: str = Query(..., description="Curated RSS token (scope=curated/global)"),
):
    return _serve_curated_feed(request, token, CURATED_DIGEST_FEED, 10)


@app.get("/")
async def root():
    """API ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸"""
    return {
        "message": "Threads Collector API",
        "version": "0.4.0",
        "endpoints": {
            "GET /scrape": "â­ ë‹¨ì¼ ê³„ì • ìŠ¤í¬ë˜í•‘ - ë©”íƒ€ë°ì´í„° í¬í•¨",
            "POST /batch-scrape": "â­ ë°°ì¹˜ ìŠ¤í¬ë˜í•‘ - ì—¬ëŸ¬ ê³„ì • ë™ì‹œ ì²˜ë¦¬ (ìµœëŒ€ 5ê°œ ë³‘ë ¬)",
            "POST /v2/scrape": "ğŸ§¼ ìƒˆ ì‘ë‹µ í˜•ì‹ - ë‹¨ì¼ ê³„ì • ìŠ¤í¬ë˜í•‘",
            "POST /v2/batch-scrape": "ğŸ§¼ ìƒˆ ì‘ë‹µ í˜•ì‹ - ë°°ì¹˜ ìŠ¤í¬ë˜í•‘",
            "GET /v2/rss": "ğŸ§¾ ê³„ì •ë³„ RSS í”¼ë“œ (token í•„ìš”)",
            "GET /v2/rss/curated": "ğŸ§  AI íë ˆì´ì…˜ RSS (10ê°œ item)",
            "GET /v2/rss/curated/digest": "ğŸ§  AI íë ˆì´ì…˜ Digest RSS (1ê°œ item)",
            "GET /admin": "ğŸ” ê´€ë¦¬ì UI",
            "GET /admin/curation": "ğŸ” íë ˆì´ì…˜ ìŠ¹ì¸/ë°œí–‰ UI",
            "GET /search-users": "ì‚¬ìš©ì ê²€ìƒ‰ (ìë™ì™„ì„±)",
            "GET /scrape-thread": "ê°œë³„ ê²Œì‹œë¬¼ê³¼ ë‹µê¸€ ìˆ˜ì§‘",
            "GET /health": "ì„œë²„ ìƒíƒœ í™•ì¸",
        },
        "quick_start": {
            "single": "GET /scrape?username=zuck&since_days=7",
            "batch": "POST /batch-scrape {\"usernames\": [\"zuck\", \"meta\"], \"since_days\": 7}",
        },
        "response_structure": {
            "single": "{ meta, stats, date_range, filter, posts }",
            "batch": "{ total_meta, results: [{ meta, stats, ... }, ...] }",
            "v2": "{ meta, filter, stats, data }",
        },
    }


@app.get("/admin")
def admin_ui(credentials: HTTPBasicCredentials = Depends(security)):
    _require_admin(credentials)
    return FileResponse("admin/index.html")


@app.get("/admin/tokens")
def admin_tokens_ui(credentials: HTTPBasicCredentials = Depends(security)):
    _require_admin(credentials)
    return FileResponse("admin/tokens.html")


@app.get("/admin/settings")
def admin_settings_ui(credentials: HTTPBasicCredentials = Depends(security)):
    _require_admin(credentials)
    return FileResponse("admin/settings.html")


@app.get("/admin/posts")
def admin_posts_ui(credentials: HTTPBasicCredentials = Depends(security)):
    _require_admin(credentials)
    return FileResponse("admin/posts.html")


@app.get("/admin/curation")
def admin_curation_ui(credentials: HTTPBasicCredentials = Depends(security)):
    _require_admin(credentials)
    return FileResponse("admin/curation.html")


@app.get("/rss.xsl")
def rss_xsl():
    headers = {
        "Cache-Control": "no-store, no-cache, must-revalidate, max-age=0",
        "Pragma": "no-cache",
    }
    return FileResponse("admin/rss.xsl", media_type="text/xsl", headers=headers)


@app.get("/admin/db")
def admin_db_ui(credentials: HTTPBasicCredentials = Depends(security)):
    _require_admin(credentials)
    return FileResponse("admin/db.html")


@app.get("/admin/api/accounts")
def admin_list_accounts(credentials: HTTPBasicCredentials = Depends(security)):
    _require_admin(credentials)
    conn = _get_db_conn()
    try:
        rows = conn.execute(
            "SELECT id, username, display_name, profile_url, is_active, include_replies, max_reply_depth, created_at "
            "FROM feed_sources ORDER BY id ASC"
        ).fetchall()
        data = [
            {
                "id": r[0],
                "username": r[1],
                "display_name": r[2],
                "profile_url": r[3],
                "is_active": bool(r[4]),
                "include_replies": bool(r[5]),
                "max_reply_depth": r[6],
                "created_at": r[7],
            }
            for r in rows
        ]
        return {"accounts": data}
    finally:
        conn.close()


@app.post("/admin/api/accounts")
def admin_add_account(payload: AdminAccountCreate, credentials: HTTPBasicCredentials = Depends(security)):
    _require_admin(credentials)
    username = payload.username.lstrip("@").strip()
    if not username:
        raise HTTPException(status_code=400, detail="username is required")
    profile_url = payload.profile_url or f"https://www.threads.com/@{username}"
    conn = _get_db_conn()
    try:
        now = datetime.now(timezone.utc).isoformat()
        conn.execute(
            "INSERT OR IGNORE INTO feed_sources "
            "(username, display_name, profile_url, include_replies, max_reply_depth, created_at) "
            "VALUES (?, ?, ?, ?, ?, ?)",
            (
                username,
                payload.display_name,
                profile_url,
                1 if payload.include_replies else 0,
                payload.max_reply_depth or 1,
                now,
            ),
        )
        conn.commit()
        return {"status": "ok"}
    finally:
        conn.close()


@app.patch("/admin/api/accounts/{account_id}")
def admin_update_account(
    account_id: int,
    payload: AdminAccountUpdate,
    credentials: HTTPBasicCredentials = Depends(security),
):
    _require_admin(credentials)
    fields = []
    values = []
    if payload.is_active is not None:
        fields.append("is_active = ?")
        values.append(1 if payload.is_active else 0)
    if payload.include_replies is not None:
        fields.append("include_replies = ?")
        values.append(1 if payload.include_replies else 0)
    if payload.max_reply_depth is not None:
        fields.append("max_reply_depth = ?")
        values.append(payload.max_reply_depth)
    if payload.display_name is not None:
        fields.append("display_name = ?")
        values.append(payload.display_name)
    if payload.profile_url is not None:
        fields.append("profile_url = ?")
        values.append(payload.profile_url)
    if not fields:
        return {"status": "no_change"}
    values.append(account_id)
    conn = _get_db_conn()
    try:
        conn.execute(
            f"UPDATE feed_sources SET {', '.join(fields)} WHERE id = ?",
            tuple(values),
        )
        conn.commit()
        return {"status": "ok"}
    finally:
        conn.close()


@app.delete("/admin/api/accounts/{account_id}")
def admin_delete_account(account_id: int, credentials: HTTPBasicCredentials = Depends(security)):
    _require_admin(credentials)
    conn = _get_db_conn()
    try:
        conn.execute("DELETE FROM feed_sources WHERE id = ?", (account_id,))
        conn.commit()
        return {"status": "ok"}
    finally:
        conn.close()


@app.post("/admin/api/scrape")
def admin_scrape(
    payload: AdminScrapeRequest,
    background_tasks: BackgroundTasks,
    credentials: HTTPBasicCredentials = Depends(security),
):
    _require_admin(credentials)
    if payload.all_accounts:
        background_tasks.add_task(rss_run_once, None, payload.max_total_posts)
    else:
        if not payload.usernames:
            raise HTTPException(status_code=400, detail="usernames is required when all_accounts=false")
        background_tasks.add_task(rss_run_once, payload.usernames, payload.max_total_posts)
    return {"status": "started"}


@app.get("/admin/api/logs")
def admin_logs(lines: int = 200, credentials: HTTPBasicCredentials = Depends(security)):
    _require_admin(credentials)
    text = _tail_log(RSS_LOG_PATH, lines=lines)
    return PlainTextResponse(text)


@app.post("/admin/api/rss-cache/refresh")
def admin_refresh_rss_cache(
    payload: AdminCacheRefreshRequest,
    credentials: HTTPBasicCredentials = Depends(security),
):
    _require_admin(credentials)
    conn = _get_db_conn()
    try:
        start_ts = datetime.now(timezone.utc)
        accounts = load_accounts(conn, only_active=True)
        if payload.all_accounts:
            targets = accounts
        else:
            if not payload.usernames:
                raise HTTPException(status_code=400, detail="usernames is required when all_accounts=false")
            usernames_set = {u.lstrip("@") for u in payload.usernames}
            targets = [a for a in accounts if a["username"] in usernames_set]
        for acc in targets:
            # delete existing cache first, then recreate
            conn.execute("DELETE FROM rss_feed_cache WHERE username = ?", (acc["username"],))
            refresh_rss_cache_for_account(conn, acc["id"], acc["username"])
        conn.commit()
        duration = int((datetime.now(timezone.utc) - start_ts).total_seconds())
        _append_log(
            f"Cache refresh: accounts={len(targets)} duration_sec={duration} "
            f"targets={[a['username'] for a in targets]}"
        )
        return {"status": "ok", "updated": len(targets)}
    finally:
        conn.close()


ALLOWED_DB_TABLES = {
    "feed_sources",
    "posts",
    "tokens",
    "rss_token_logs",
    "rss_invalid_token_logs",
    "rss_rate_limits",
    "rss_token_counters",
    "rss_feed_cache",
    "rss_cache_policy",
    "rss_schedule",
    "curation_config",
    "curation_runs",
    "curation_candidates",
    "curation_llm_cache",
    "curation_publications",
    "curation_publication_items",
    "curated_rss_cache",
}


@app.get("/admin/api/db/tables")
def admin_db_tables(credentials: HTTPBasicCredentials = Depends(security)):
    _require_admin(credentials)
    return {"tables": sorted(ALLOWED_DB_TABLES)}


@app.get("/admin/api/db/rows")
def admin_db_rows(
    table: str,
    limit: int = 50,
    offset: int = 0,
    credentials: HTTPBasicCredentials = Depends(security),
):
    _require_admin(credentials)
    if table not in ALLOWED_DB_TABLES:
        raise HTTPException(status_code=400, detail="invalid table")
    limit = max(1, min(500, limit))
    offset = max(0, offset)
    conn = _get_db_conn()
    try:
        cols = conn.execute(f"PRAGMA table_info({table})").fetchall()
        columns = [c[1] for c in cols]
        total = conn.execute(f"SELECT COUNT(*) FROM {table}").fetchone()[0]
        rows = conn.execute(
            f"SELECT * FROM {table} LIMIT ? OFFSET ?",
            (limit, offset),
        ).fetchall()
        return {"columns": columns, "rows": rows, "total": total}
    finally:
        conn.close()


class AdminDbDelete(BaseModel):
    table: str


@app.post("/admin/api/db/delete")
def admin_db_delete(
    payload: AdminDbDelete,
    credentials: HTTPBasicCredentials = Depends(security),
):
    _require_admin(credentials)
    table = payload.table
    if table not in ALLOWED_DB_TABLES:
        raise HTTPException(status_code=400, detail="invalid table")
    conn = _get_db_conn()
    try:
        conn.execute(f"DELETE FROM {table}")
        conn.commit()
        return {"status": "ok"}
    finally:
        conn.close()


@app.get("/admin/api/posts")
def admin_list_posts(
    username: Optional[str] = None,
    type: Optional[str] = None,
    limit: int = 50,
    offset: int = 0,
    credentials: HTTPBasicCredentials = Depends(security),
):
    _require_admin(credentials)
    limit = max(1, min(500, limit))
    offset = max(0, offset)
    username_value = username.lstrip("@").strip() if username else None
    type_value = (type or "all").lower()
    where = []
    params: List[Any] = []
    if username_value:
        where.append("s.username = ?")
        params.append(username_value)
    if type_value == "root":
        where.append("p.is_reply = 0")
    elif type_value == "reply":
        where.append("p.is_reply = 1")
    where_sql = f"WHERE {' AND '.join(where)}" if where else ""
    conn = _get_db_conn()
    try:
        total = conn.execute(
            f"SELECT COUNT(*) FROM posts p JOIN feed_sources s ON s.id = p.source_id {where_sql}",
            tuple(params),
        ).fetchone()[0]
        rows = conn.execute(
            "SELECT p.post_id, p.url, p.text, p.media_json, p.created_at, p.is_reply, s.username "
            "FROM posts p JOIN feed_sources s ON s.id = p.source_id "
            f"{where_sql} ORDER BY p.created_at DESC LIMIT ? OFFSET ?",
            tuple(params + [limit, offset]),
        ).fetchall()
        return {
            "total": total,
            "posts": [
                {
                    "post_id": r[0],
                    "url": r[1],
                    "text": r[2],
                    "media": _parse_media_json(r[3]),
                    "created_at": r[4],
                    "is_reply": bool(r[5]),
                    "username": r[6],
                }
                for r in rows
            ],
        }
    finally:
        conn.close()


@app.get("/admin/api/schedule")
def admin_get_schedule(credentials: HTTPBasicCredentials = Depends(security)):
    _require_admin(credentials)
    conn = _get_db_conn()
    try:
        sched = _get_schedule(conn, display_kst=True)
        return {**sched, "next_run_at": _compute_next_run(sched, sched.get("start_time_utc") or "00:00")}
    finally:
        conn.close()


@app.patch("/admin/api/schedule")
def admin_update_schedule(
    payload: AdminScheduleUpdate,
    credentials: HTTPBasicCredentials = Depends(security),
):
    _require_admin(credentials)
    conn = _get_db_conn()
    try:
        fields = []
        values = []
        interval_minutes = None
        if payload.is_active is not None:
            fields.append("is_active = ?")
            values.append(1 if payload.is_active else 0)
        if payload.interval_minutes is not None:
            interval_minutes = payload.interval_minutes
            fields.append("interval_minutes = ?")
            values.append(payload.interval_minutes)
        if payload.start_time is not None:
            # Convert KST HH:MM -> UTC HH:MM for storage
            try:
                hh, mm = payload.start_time.split(":")
                kst_dt = datetime(2000, 1, 1, int(hh), int(mm), tzinfo=KST)
                utc_dt = kst_dt.astimezone(timezone.utc)
                utc_hhmm = f"{utc_dt.hour:02d}:{utc_dt.minute:02d}"
            except Exception:
                utc_hhmm = "00:00"
            fields.append("start_time = ?")
            values.append(utc_hhmm)
        if not fields:
            return {"status": "no_change"}
        now = datetime.now(timezone.utc).isoformat()
        fields.append("updated_at = ?")
        values.append(now)
        values.append(1)
        conn.execute(
            f"UPDATE rss_schedule SET {', '.join(fields)} WHERE id = ?",
            tuple(values),
        )
        conn.commit()
        if interval_minutes is not None:
            try:
                conn.execute(
                    "UPDATE rss_cache_policy SET ttl_seconds = ?, updated_at = ? WHERE id = 1",
                    (interval_minutes * 60, now),
                )
                conn.commit()
            except Exception:
                pass
        # Optional: update systemd timer if enabled
        if _systemd_allowed():
            try:
                sched = _get_schedule(conn)
                _write_systemd_timer(sched)
                subprocess.run(["systemctl", "daemon-reload"], check=False)
                subprocess.run(["systemctl", "restart", "thread-collector-rss.timer"], check=False)
            except Exception:
                pass
        return {"status": "ok"}
    finally:
        conn.close()


@app.get("/admin/api/tokens")
def admin_list_tokens(credentials: HTTPBasicCredentials = Depends(security)):
    _require_admin(credentials)
    conn = _get_db_conn()
    try:
        rows = conn.execute(
            "SELECT id, token, scope, is_active, created_at FROM tokens ORDER BY id DESC"
        ).fetchall()
        return {
            "tokens": [
                {
                    "id": r[0],
                    "token": r[1],
                    "scope": r[2],
                    "is_active": bool(r[3]),
                    "created_at": r[4],
                }
                for r in rows
            ]
        }
    finally:
        conn.close()


@app.post("/admin/api/tokens")
def admin_create_token(
    payload: AdminTokenCreate,
    credentials: HTTPBasicCredentials = Depends(security),
):
    _require_admin(credentials)
    scope = payload.scope.strip()
    if not scope:
        raise HTTPException(status_code=400, detail="scope is required")
    token = secrets.token_urlsafe(24)
    conn = _get_db_conn()
    try:
        now = datetime.now(timezone.utc).isoformat()
        conn.execute(
            "INSERT INTO tokens (token, scope, created_at, is_active) VALUES (?, ?, ?, ?)",
            (token, scope, now, 1 if payload.is_active else 0),
        )
        conn.commit()
        return {"token": token}
    finally:
        conn.close()


@app.patch("/admin/api/tokens/{token_id}")
def admin_update_token(
    token_id: int,
    is_active: bool,
    credentials: HTTPBasicCredentials = Depends(security),
):
    _require_admin(credentials)
    conn = _get_db_conn()
    try:
        conn.execute(
            "UPDATE tokens SET is_active = ? WHERE id = ?",
            (1 if is_active else 0, token_id),
        )
        conn.commit()
        return {"status": "ok"}
    finally:
        conn.close()


@app.delete("/admin/api/tokens/{token_id}")
def admin_delete_token(token_id: int, credentials: HTTPBasicCredentials = Depends(security)):
    _require_admin(credentials)
    conn = _get_db_conn()
    try:
        conn.execute("DELETE FROM tokens WHERE id = ?", (token_id,))
        conn.commit()
        return {"status": "ok"}
    finally:
        conn.close()


@app.get("/admin/api/rate-limit")
def admin_get_rate_limit(credentials: HTTPBasicCredentials = Depends(security)):
    _require_admin(credentials)
    conn = _get_db_conn()
    try:
        return _get_rate_limit(conn)
    finally:
        conn.close()


@app.patch("/admin/api/rate-limit")
def admin_update_rate_limit(
    payload: AdminRateLimitUpdate,
    credentials: HTTPBasicCredentials = Depends(security),
):
    _require_admin(credentials)
    conn = _get_db_conn()
    try:
        fields = []
        values = []
        if payload.window_seconds is not None:
            fields.append("window_seconds = ?")
            values.append(payload.window_seconds)
        if payload.max_requests is not None:
            fields.append("max_requests = ?")
            values.append(payload.max_requests)
        if not fields:
            return {"status": "no_change"}
        now = datetime.now(timezone.utc).isoformat()
        fields.append("updated_at = ?")
        values.append(now)
        values.append(1)
        conn.execute(
            f"UPDATE rss_rate_limits SET {', '.join(fields)} WHERE id = ?",
            tuple(values),
        )
        conn.commit()
        return {"status": "ok"}
    finally:
        conn.close()


@app.get("/admin/api/token-logs")
def admin_token_logs(
    limit: int = 200,
    credentials: HTTPBasicCredentials = Depends(security),
):
    _require_admin(credentials)
    conn = _get_db_conn()
    try:
        rows = conn.execute(
            "SELECT l.id, l.token_id, t.scope, l.username, l.ip, l.user_agent, l.status_code, l.created_at "
            "FROM rss_token_logs l "
            "JOIN tokens t ON t.id = l.token_id "
            "ORDER BY l.id DESC LIMIT ?",
            (limit,),
        ).fetchall()
        return {
            "logs": [
                {
                    "id": r[0],
                    "token_id": r[1],
                    "scope": r[2],
                    "username": r[3],
                    "ip": r[4],
                    "user_agent": r[5],
                    "status_code": r[6],
                    "created_at": r[7],
                }
                for r in rows
            ]
        }
    finally:
        conn.close()


@app.get("/admin/api/token-stats")
def admin_token_stats(
    hours: int = 24,
    credentials: HTTPBasicCredentials = Depends(security),
):
    _require_admin(credentials)
    if hours < 1 or hours > 168:
        raise HTTPException(status_code=400, detail="hours must be 1~168")
    since = datetime.now(timezone.utc) - timedelta(hours=hours)
    conn = _get_db_conn()
    try:
        rows = conn.execute(
            "SELECT t.id, t.scope, COUNT(*) as cnt "
            "FROM rss_token_logs l "
            "JOIN tokens t ON t.id = l.token_id "
            "WHERE l.created_at >= ? "
            "GROUP BY t.id, t.scope "
            "ORDER BY cnt DESC",
            (since.isoformat(),),
        ).fetchall()
        return {
            "hours": hours,
            "stats": [
                {"token_id": r[0], "scope": r[1], "count": r[2]}
                for r in rows
            ],
        }
    finally:
        conn.close()


@app.get("/admin/api/invalid-token-logs")
def admin_invalid_token_logs(
    limit: int = 200,
    credentials: HTTPBasicCredentials = Depends(security),
):
    _require_admin(credentials)
    conn = _get_db_conn()
    try:
        rows = conn.execute(
            "SELECT id, token_hash, username, ip, user_agent, status_code, created_at "
            "FROM rss_invalid_token_logs ORDER BY id DESC LIMIT ?",
            (limit,),
        ).fetchall()
        return {
            "logs": [
                {
                    "id": r[0],
                    "token_hash": r[1],
                    "username": r[2],
                    "ip": r[3],
                    "user_agent": r[4],
                    "status_code": r[5],
                    "created_at": r[6],
                }
                for r in rows
            ]
        }
    finally:
        conn.close()


@app.post("/admin/api/rss-cache/clear")
def admin_clear_rss_cache(credentials: HTTPBasicCredentials = Depends(security)):
    _require_admin(credentials)
    conn = _get_db_conn()
    try:
        conn.execute("DELETE FROM rss_feed_cache")
        conn.commit()
        return {"status": "ok"}
    finally:
        conn.close()


@app.get("/admin/api/rss-cache-policy")
def admin_get_rss_cache_policy(credentials: HTTPBasicCredentials = Depends(security)):
    _require_admin(credentials)
    conn = _get_db_conn()
    try:
        return _get_cache_policy(conn)
    finally:
        conn.close()


@app.patch("/admin/api/rss-cache-policy")
def admin_update_rss_cache_policy(
    payload: AdminCachePolicyUpdate,
    credentials: HTTPBasicCredentials = Depends(security),
):
    _require_admin(credentials)
    conn = _get_db_conn()
    try:
        fields = []
        values = []
        if payload.ttl_seconds is not None:
            fields.append("ttl_seconds = ?")
            values.append(payload.ttl_seconds)
        if payload.enabled is not None:
            fields.append("enabled = ?")
            values.append(1 if payload.enabled else 0)
        if not fields:
            return {"status": "no_change"}
        now = datetime.now(timezone.utc).isoformat()
        fields.append("updated_at = ?")
        values.append(now)
        values.append(1)
        conn.execute(
            f"UPDATE rss_cache_policy SET {', '.join(fields)} WHERE id = ?",
            tuple(values),
        )
        conn.commit()
        return {"status": "ok"}
    finally:
        conn.close()


@app.get("/admin/api/curation/config")
def admin_get_curation_config(credentials: HTTPBasicCredentials = Depends(security)):
    _require_admin(credentials)
    conn = _get_db_conn()
    try:
        cfg = curation_get_config(conn)
        key = str(cfg.get("openrouter_api_key") or "").strip()
        if "openrouter_api_key" in cfg:
            del cfg["openrouter_api_key"]
        cfg["has_openrouter_api_key"] = bool(key)
        return cfg
    finally:
        conn.close()


@app.patch("/admin/api/curation/config")
def admin_update_curation_config(
    payload: CurationConfigUpdate,
    credentials: HTTPBasicCredentials = Depends(security),
):
    _require_admin(credentials)
    updates = payload.model_dump(exclude_none=True)
    if "daily_run_time_kst" in updates:
        value = str(updates["daily_run_time_kst"] or "").strip()
        try:
            hh, mm = value.split(":")
            hh_i = int(hh)
            mm_i = int(mm)
            if hh_i < 0 or hh_i > 23 or mm_i < 0 or mm_i > 59:
                raise ValueError
            updates["daily_run_time_kst"] = f"{hh_i:02d}:{mm_i:02d}"
        except Exception:
            raise HTTPException(status_code=400, detail="daily_run_time_kst must be HH:MM")
    conn = _get_db_conn()
    try:
        return curation_update_config(conn, updates)
    finally:
        conn.close()


@app.post("/admin/api/curation/run")
def admin_run_curation(
    payload: CurationRunRequest,
    credentials: HTTPBasicCredentials = Depends(security),
):
    _require_admin(credentials)
    conn = _get_db_conn()
    try:
        run_result = curation_run_once(conn, force=payload.force, logger=_append_log)
        auto_pub_result = curation_maybe_auto_publish_due(conn, logger=_append_log)
        if auto_pub_result:
            curation_refresh_cache(conn, CURATED_ITEM_FEED, 10)
            curation_refresh_cache(conn, CURATED_DIGEST_FEED, 10)
            _append_log("[curation] cache refreshed after auto publish")
        return {"status": "ok", "run": run_result, "auto_publish": auto_pub_result}
    finally:
        conn.close()


@app.post("/admin/api/curation/stage1/list")
def admin_curation_stage1_list(
    payload: CurationStage1Request,
    credentials: HTTPBasicCredentials = Depends(security),
):
    _require_admin(credentials)
    conn = _get_db_conn()
    try:
        result = curation_create_candidate_list_once(
            conn,
            hours=payload.hours,
            force=payload.force,
            logger=_append_log,
        )
        return {"status": "ok", **result}
    finally:
        conn.close()


@app.post("/admin/api/curation/stage2/score")
def admin_curation_stage2_score(
    payload: CurationStageRunRequest,
    credentials: HTTPBasicCredentials = Depends(security),
):
    _require_admin(credentials)
    conn = _get_db_conn()
    try:
        result = curation_score_candidates_once(conn, run_id=payload.run_id, logger=_append_log)
        return {"status": "ok", **result}
    finally:
        conn.close()


@app.post("/admin/api/curation/stage3/summarize")
def admin_curation_stage3_summarize(
    payload: CurationStage3Request,
    credentials: HTTPBasicCredentials = Depends(security),
):
    _require_admin(credentials)
    conn = _get_db_conn()
    try:
        result = curation_summarize_candidates_once(
            conn,
            run_id=payload.run_id,
            candidate_id=payload.candidate_id,
            force=payload.force,
            logger=_append_log,
        )
        return {"status": "ok", **result}
    finally:
        conn.close()


@app.get("/admin/api/curation/runs")
def admin_list_curation_runs(
    limit: int = 30,
    credentials: HTTPBasicCredentials = Depends(security),
):
    _require_admin(credentials)
    conn = _get_db_conn()
    try:
        return {"runs": curation_list_runs(conn, limit=limit)}
    finally:
        conn.close()


@app.get("/admin/api/curation/candidates")
def admin_list_curation_candidates(
    run_id: Optional[int] = None,
    credentials: HTTPBasicCredentials = Depends(security),
):
    _require_admin(credentials)
    conn = _get_db_conn()
    try:
        return curation_list_candidates(conn, run_id)
    finally:
        conn.close()


@app.get("/admin/api/curation/preview")
def admin_curation_preview(
    run_id: int,
    limit: int = 10,
    credentials: HTTPBasicCredentials = Depends(security),
):
    _require_admin(credentials)
    conn = _get_db_conn()
    try:
        return curation_preview_run(conn, run_id=run_id, limit=limit)
    finally:
        conn.close()


@app.patch("/admin/api/curation/candidates/{candidate_id}")
def admin_update_curation_candidate(
    candidate_id: int,
    payload: CurationCandidateUpdate,
    credentials: HTTPBasicCredentials = Depends(security),
):
    _require_admin(credentials)
    updates = payload.model_dump(exclude_none=True)
    conn = _get_db_conn()
    try:
        curation_update_candidate(conn, candidate_id, updates, reviewed_by=credentials.username)
        return {"status": "ok"}
    finally:
        conn.close()


@app.delete("/admin/api/curation/candidates/{candidate_id}")
def admin_delete_curation_candidate(
    candidate_id: int,
    credentials: HTTPBasicCredentials = Depends(security),
):
    _require_admin(credentials)
    conn = _get_db_conn()
    try:
        conn.execute("DELETE FROM curation_candidates WHERE id = ?", (candidate_id,))
        conn.commit()
        return {"status": "ok"}
    finally:
        conn.close()


@app.post("/admin/api/curation/approve-all")
def admin_curation_approve_all(
    payload: CurationApproveAllRequest,
    credentials: HTTPBasicCredentials = Depends(security),
):
    _require_admin(credentials)
    conn = _get_db_conn()
    try:
        curation_approve_all(conn, payload.run_id, reviewed_by=credentials.username)
        return {"status": "ok"}
    finally:
        conn.close()


@app.post("/admin/api/curation/reorder")
def admin_curation_reorder(
    payload: CurationReorderRequest,
    credentials: HTTPBasicCredentials = Depends(security),
):
    _require_admin(credentials)
    ordered_ids = []
    seen = set()
    for cid in payload.ordered_ids:
        if cid in seen:
            continue
        seen.add(cid)
        ordered_ids.append(cid)
    if not ordered_ids:
        raise HTTPException(status_code=400, detail="ordered_ids is empty")
    conn = _get_db_conn()
    try:
        curation_reorder_candidates(conn, payload.run_id, ordered_ids)
        return {"status": "ok", "count": len(ordered_ids)}
    finally:
        conn.close()


@app.post("/admin/api/curation/publish")
def admin_curation_publish(
    payload: CurationPublishRequest,
    credentials: HTTPBasicCredentials = Depends(security),
):
    _require_admin(credentials)
    conn = _get_db_conn()
    try:
        run_id = payload.run_id or curation_latest_ready_run(conn)
        if not run_id:
            raise HTTPException(status_code=400, detail="no ready run")
        result = curation_publish_run(
            conn,
            run_id,
            publish_mode=payload.publish_mode or "manual",
            published_by=credentials.username,
            logger=_append_log,
        )
        curation_refresh_cache(conn, CURATED_ITEM_FEED, 10)
        curation_refresh_cache(conn, CURATED_DIGEST_FEED, 10)
        _append_log(f"[curation] manual publish run_id={run_id} and cache refreshed")
        return {"status": "ok", "run_id": run_id, **result}
    finally:
        conn.close()


@app.post("/admin/api/curation/cache/refresh")
def admin_curation_refresh_cache(
    payload: CurationCacheRefreshRequest,
    credentials: HTTPBasicCredentials = Depends(security),
):
    _require_admin(credentials)
    conn = _get_db_conn()
    try:
        targets: List[str] = []
        if payload.feed_type in ("all", "item"):
            targets.append(CURATED_ITEM_FEED)
        if payload.feed_type in ("all", "digest"):
            targets.append(CURATED_DIGEST_FEED)
        refreshed: Dict[str, Dict[str, Any]] = {}
        for feed_type in targets:
            refreshed[feed_type] = curation_refresh_cache(conn, feed_type, payload.limit)
        _append_log(
            f"[curation] cache refresh feed_type={payload.feed_type} limit={payload.limit} targets={targets}"
        )
        return {"status": "ok", "refreshed": refreshed}
    finally:
        conn.close()


@app.get("/admin/api/curation/publication")
def admin_curation_publication(credentials: HTTPBasicCredentials = Depends(security)):
    _require_admin(credentials)
    conn = _get_db_conn()
    try:
        return {"publication": curation_latest_publication(conn)}
    finally:
        conn.close()


@app.get("/admin/api/curation/stats")
def admin_curation_stats(
    days: int = 30,
    credentials: HTTPBasicCredentials = Depends(security),
):
    _require_admin(credentials)
    conn = _get_db_conn()
    try:
        data = curation_stats(conn, days=days)
        cfg = curation_get_config(conn)
        runs = curation_list_runs(conn, limit=1)
        data["target_daily_spend_usd"] = float(cfg.get("target_daily_spend_usd", 1.0))
        data["latest_run_over_target"] = bool(runs[0]["budget_over_target"]) if runs else False
        return data
    finally:
        conn.close()


@app.get("/admin/api/token-logs.csv")
def admin_token_logs_csv(credentials: HTTPBasicCredentials = Depends(security)):
    _require_admin(credentials)
    conn = _get_db_conn()
    try:
        rows = conn.execute(
            "SELECT l.id, l.token_id, t.scope, l.username, l.ip, l.user_agent, l.status_code, l.created_at "
            "FROM rss_token_logs l "
            "JOIN tokens t ON t.id = l.token_id "
            "ORDER BY l.id DESC"
        ).fetchall()
        lines = [
            "id,token_id,scope,username,ip,user_agent,status_code,created_at"
        ]
        for r in rows:
            row = [
                str(r[0]),
                str(r[1]),
                r[2] or "",
                r[3] or "",
                r[4] or "",
                (r[5] or "").replace("\n", " ").replace("\r", " "),
                str(r[6]),
                r[7] or "",
            ]
            lines.append(",".join([f"\"{c}\"" if "," in c else c for c in row]))
        csv_text = "\n".join(lines)
        return PlainTextResponse(csv_text, media_type="text/csv")
    finally:
        conn.close()


def _systemd_allowed() -> bool:
    return os.environ.get("ENABLE_SYSTEMD_CONTROL") == "1"

def _write_systemd_timer(schedule: Dict[str, Any]) -> None:
    interval_minutes = schedule.get("interval_minutes", 30)
    start_time_utc = schedule.get("start_time_utc") or schedule.get("start_time") or "00:00"
    timer_path = os.environ.get(
        "SYSTEMD_TIMER_PATH",
        "/etc/systemd/system/thread-collector-rss.timer",
    )
    on_calendar = None
    try:
        hh, mm = start_time_utc.split(":")
        hh_i = int(hh)
        mm_i = int(mm)
    except Exception:
        hh_i, mm_i = 0, 0

    if interval_minutes > 0 and 60 % interval_minutes == 0:
        # Run from start_time hour to end of day, aligned by minute offset
        on_calendar = f"*-*-* {hh_i:02d}..23:{mm_i:02d}/{interval_minutes}"
        content = (
            "[Unit]\n"
            "Description=Thread Collector RSS Sync Timer\n\n"
            "[Timer]\n"
            f"OnCalendar={on_calendar}\n"
            "AccuracySec=1min\n"
            "Persistent=true\n\n"
            "[Install]\n"
            "WantedBy=timers.target\n"
        )
    else:
        # Fallback: start at specific time, then repeat by interval
        on_calendar = f"*-*-* {hh_i:02d}:{mm_i:02d}:00"
        content = (
            "[Unit]\n"
            "Description=Thread Collector RSS Sync Timer\n\n"
            "[Timer]\n"
            f"OnCalendar={on_calendar}\n"
            f"OnUnitActiveSec={interval_minutes}min\n"
            "AccuracySec=1min\n"
            "Persistent=true\n\n"
            "[Install]\n"
            "WantedBy=timers.target\n"
        )
    with open(timer_path, "w", encoding="utf-8") as f:
        f.write(content)


@app.get("/admin/api/systemd/status")
def admin_systemd_status(credentials: HTTPBasicCredentials = Depends(security)):
    _require_admin(credentials)
    if not _systemd_allowed():
        raise HTTPException(status_code=403, detail="systemd control disabled")
    res = subprocess.run(
        ["systemctl", "status", "thread-collector-rss.timer", "--no-pager"],
        capture_output=True,
        text=True,
    )
    return PlainTextResponse(res.stdout + res.stderr)


@app.post("/admin/api/systemd/start")
def admin_systemd_start(credentials: HTTPBasicCredentials = Depends(security)):
    _require_admin(credentials)
    if not _systemd_allowed():
        raise HTTPException(status_code=403, detail="systemd control disabled")
    subprocess.run(["systemctl", "start", "thread-collector-rss.timer"], check=False)
    return {"status": "ok"}


@app.post("/admin/api/systemd/stop")
def admin_systemd_stop(credentials: HTTPBasicCredentials = Depends(security)):
    _require_admin(credentials)
    if not _systemd_allowed():
        raise HTTPException(status_code=403, detail="systemd control disabled")
    subprocess.run(["systemctl", "stop", "thread-collector-rss.timer"], check=False)
    return {"status": "ok"}


@app.post("/admin/api/systemd/restart")
def admin_systemd_restart(credentials: HTTPBasicCredentials = Depends(security)):
    _require_admin(credentials)
    if not _systemd_allowed():
        raise HTTPException(status_code=403, detail="systemd control disabled")
    subprocess.run(["systemctl", "restart", "thread-collector-rss.timer"], check=False)
    return {"status": "ok"}

@app.get("/health")
async def health():
    """ì„œë²„ ìƒíƒœ í™•ì¸"""
    return {"status": "healthy"}


# ============ ë‹µê¸€ í¬í•¨ ìŠ¤í¬ë˜í•‘ ì—”ë“œí¬ì¸íŠ¸ ============

@app.get("/scrape-thread", response_model=ThreadWithRepliesResponse)
async def scrape_thread_get(
    url: str = Query(..., description="ê²Œì‹œë¬¼ URL (ì˜ˆ: 'https://www.threads.com/@user/post/ABC123')"),
    max_depth: int = Query(1, description="ìµœëŒ€ ë‹µê¸€ ê¹Šì´ (ê¸°ë³¸: ë£¨íŠ¸ + ì§ì ‘ reply)", ge=1, le=10),
    max_replies: int = Query(100, description="ê° ë ˆë²¨ë‹¹ ìµœëŒ€ ë‹µê¸€ ìˆ˜", ge=1, le=500),
):
    """GET ë°©ì‹ìœ¼ë¡œ ê°œë³„ ê²Œì‹œë¬¼ê³¼ ëª¨ë“  ë‹µê¸€ ìˆ˜ì§‘
    
    ì˜ˆì‹œ:
    - GET /scrape-thread?url=https://www.threads.com/@zuck/post/ABC123
    - GET /scrape-thread?url=https://www.threads.com/@zuck/post/ABC123&max_depth=3
    """
    try:
        thread_data = await scrape_thread_with_replies(
            post_url=url,
            max_depth=max_depth,
            max_replies_per_level=max_replies,
        )
        
        # ReplyPost ë³€í™˜
        def convert_replies(replies: List[Dict]) -> List[ReplyPost]:
            result = []
            for r in replies:
                result.append(ReplyPost(
                    post_id=r.get("post_id"),
                    text=r.get("text", ""),
                    created_at=r.get("created_at"),
                    url=r.get("url", ""),
                    author=r.get("author"),
                    media=r.get("media", []),
                    replies=convert_replies(r.get("replies", [])),
                ))
            return result
        
        return ThreadWithRepliesResponse(
            post_id=thread_data.get("post_id"),
            text=thread_data.get("text", ""),
            created_at=thread_data.get("created_at"),
            url=thread_data.get("url", ""),
            author=thread_data.get("author"),
            media=thread_data.get("media", []),
            replies=convert_replies(thread_data.get("replies", [])),
            total_replies_count=thread_data.get("total_replies_count", 0),
            scraped_at=datetime.now(KST).isoformat(),
        )
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        print(f"ìŠ¤ë ˆë“œ ìŠ¤í¬ë˜í•‘ ì˜¤ë¥˜:")
        print(traceback.format_exc())
        raise HTTPException(status_code=500, detail=f"ìŠ¤ë ˆë“œ ìŠ¤í¬ë˜í•‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")


@app.post("/scrape-thread", response_model=ThreadWithRepliesResponse)
async def scrape_thread_post(request: ScrapeThreadRequest):
    """POST ë°©ì‹ìœ¼ë¡œ ê°œë³„ ê²Œì‹œë¬¼ê³¼ ëª¨ë“  ë‹µê¸€ ìˆ˜ì§‘
    
    ì˜ˆì‹œ:
    ```json
    {
        "post_url": "https://www.threads.com/@zuck/post/ABC123",
        "max_depth": 5,
        "max_replies_per_level": 100
    }
    ```
    """
    try:
        thread_data = await scrape_thread_with_replies(
            post_url=request.post_url,
            max_depth=request.max_depth,
            max_replies_per_level=request.max_replies_per_level,
        )
        
        # ReplyPost ë³€í™˜
        def convert_replies(replies: List[Dict]) -> List[ReplyPost]:
            result = []
            for r in replies:
                result.append(ReplyPost(
                    post_id=r.get("post_id"),
                    text=r.get("text", ""),
                    created_at=r.get("created_at"),
                    url=r.get("url", ""),
                    author=r.get("author"),
                    media=r.get("media", []),
                    replies=convert_replies(r.get("replies", [])),
                ))
            return result
        
        return ThreadWithRepliesResponse(
            post_id=thread_data.get("post_id"),
            text=thread_data.get("text", ""),
            created_at=thread_data.get("created_at"),
            url=thread_data.get("url", ""),
            author=thread_data.get("author"),
            media=thread_data.get("media", []),
            replies=convert_replies(thread_data.get("replies", [])),
            total_replies_count=thread_data.get("total_replies_count", 0),
            scraped_at=datetime.now(KST).isoformat(),
        )
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        print(f"ìŠ¤ë ˆë“œ ìŠ¤í¬ë˜í•‘ ì˜¤ë¥˜:")
        print(traceback.format_exc())
        raise HTTPException(status_code=500, detail=f"ìŠ¤ë ˆë“œ ìŠ¤í¬ë˜í•‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")


@app.post("/scrape-with-replies", response_model=ScrapeWithRepliesResponse)
async def scrape_profile_with_replies(request: ScrapeWithRepliesRequest):
    """í”„ë¡œí•„ì˜ ê²Œì‹œë¬¼ê³¼ ê° ê²Œì‹œë¬¼ì˜ ë‹µê¸€ì„ ëª¨ë‘ ìˆ˜ì§‘
    
    âš ï¸ ì£¼ì˜: ë‹µê¸€ ìˆ˜ì§‘ ì‹œ ê° ê²Œì‹œë¬¼ë§ˆë‹¤ í˜ì´ì§€ ë°©ë¬¸ì´ í•„ìš”í•˜ë¯€ë¡œ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    
    ì˜ˆì‹œ:
    ```json
    {
        "username": "zuck",
        "max_posts": 5,
        "include_replies": true,
        "max_reply_depth": 3
    }
    ```
    """
    try:
        username = request.username.lstrip("@")
        
        if not username:
            raise HTTPException(status_code=400, detail="ì‚¬ìš©ìëª…ì´ í•„ìš”í•©ë‹ˆë‹¤")
        
        posts = await scrape_threads_profile_with_replies(
            username=username,
            max_posts=request.max_posts,
            include_replies=request.include_replies,
            max_reply_depth=request.max_reply_depth,
            cutoff_utc=compute_cutoff_utc(since_days=request.since_days, since_date=request.since_date),
        )
        
        # ë‚ ì§œ í•„í„° ì ìš©
        if request.since_days or request.since_date:
            posts = filter_posts_by_date(posts, since_days=request.since_days, since_date=request.since_date)
        
        # í•„í„° ì„¤ëª… ìƒì„±
        filter_desc = None
        if request.since_days:
            filter_desc = f"ìµœê·¼ {request.since_days}ì¼ ì´ë‚´"
        elif request.since_date:
            filter_desc = f"{request.since_date} ì´í›„"
        
        # ReplyPost ë³€í™˜ í•¨ìˆ˜
        def convert_replies(replies: List[Dict]) -> List[ReplyPost]:
            result = []
            for r in replies:
                result.append(ReplyPost(
                    post_id=r.get("post_id"),
                    text=r.get("text", ""),
                    created_at=r.get("created_at"),
                    url=r.get("url", ""),
                    author=r.get("author"),
                    media=r.get("media", []),
                    replies=convert_replies(r.get("replies", [])),
                ))
            return result
        
        # ThreadWithRepliesResponse ë³€í™˜
        thread_responses = []
        for post in posts:
            thread_responses.append(ThreadWithRepliesResponse(
                post_id=post.get("post_id"),
                text=post.get("text", ""),
                created_at=post.get("created_at"),
                url=post.get("url", ""),
                author=post.get("author"),
                media=post.get("media", []),
                replies=convert_replies(post.get("replies", [])),
                total_replies_count=post.get("total_replies_count", 0),
                scraped_at=datetime.now(KST).isoformat(),
            ))
        
        return ScrapeWithRepliesResponse(
            username=username,
            total_posts=len(thread_responses),
            posts=thread_responses,
            scraped_at=datetime.now(KST).isoformat(),
            filter_applied=filter_desc,
        )
    except HTTPException:
        raise
    except Exception as e:
        print(f"í”„ë¡œí•„+ë‹µê¸€ ìŠ¤í¬ë˜í•‘ ì˜¤ë¥˜:")
        print(traceback.format_exc())
        raise HTTPException(status_code=500, detail=f"ìŠ¤í¬ë˜í•‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")


@app.get("/search-users", response_model=UserSearchResponse)
async def search_users(
    q: str = Query(..., description="ê²€ìƒ‰ì–´ (ì˜ˆ: 'cat', 'zuck')", min_length=1),
    max_results: int = Query(10, description="ìµœëŒ€ ê²°ê³¼ ìˆ˜", ge=1, le=50),
):
    """Threads ì‚¬ìš©ì ê²€ìƒ‰ (ìë™ì™„ì„±ìš©)
    
    ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•˜ë©´ ë§¤ì¹­ë˜ëŠ” Threads ì‚¬ìš©ì ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    
    ì˜ˆì‹œ:
    - GET /search-users?q=cat  â†’ cheese.cat.ai ë“± ê²€ìƒ‰
    - GET /search-users?q=zuck&max_results=5
    """
    try:
        users = await search_threads_users(query=q, max_results=max_results)
        
        return UserSearchResponse(
            query=q,
            total_results=len(users),
            users=[UserSearchResult(**user) for user in users],
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ì‚¬ìš©ì ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")


@app.get("/scrape")
async def scrape_get(
    username: str = Query(..., description="Threads ì‚¬ìš©ìëª… (ì˜ˆ: 'zuck')"),
    include_replies: bool = Query(True, description="ë‹µê¸€ í¬í•¨ ì—¬ë¶€ (ê¸°ë³¸: True)"),
    since_days: int = Query(1, description="ìµœê·¼ Nì¼ ì´ë‚´ ê²Œì‹œë¬¼ë§Œ (ê¸°ë³¸: 1ì¼)", ge=1, le=365),
    since_date: Optional[str] = Query(None, description="íŠ¹ì • ë‚ ì§œ ì´í›„ ê²Œì‹œë¬¼ë§Œ (ISO í˜•ì‹, since_daysë³´ë‹¤ ìš°ì„ )"),
    max_reply_depth: int = Query(1, description="ë‹µê¸€ ìˆ˜ì§‘ ì‹œ ìµœëŒ€ ê¹Šì´ (ê¸°ë³¸: ë£¨íŠ¸ + ì§ì ‘ reply)", ge=1, le=10),
    max_total_posts: int = Query(100, description="ì „ì²´ ì¶œë ¥ ìµœëŒ€ ê²Œì‹œë¬¼ ìˆ˜ (ëª¨ë“  root + ëª¨ë“  ë‹µê¸€ í•©ê³„, ê¸°ë³¸: 100)", ge=1, le=1000),
):
    """ì‚¬ìš©ìëª…ìœ¼ë¡œ ìŠ¤ë ˆë“œ + ëª¨ë“  ë‹µê¸€ ìˆ˜ì§‘
    
    ê¸°ë³¸ê°’:
    - ìµœê·¼ 1ì¼ ì´ë‚´ ê²Œì‹œë¬¼
    - ë‹µê¸€ í¬í•¨
    - ì „ì²´ ì¶œë ¥ ìµœëŒ€ 100ê°œ ê²Œì‹œë¬¼
    
    ì˜ˆì‹œ: 
    - GET /scrape?username=zuck  (ìµœê·¼ 1ì¼, ë‹µê¸€ í¬í•¨, ì´ 100ê°œ)
    - GET /scrape?username=zuck&since_days=7  (ìµœê·¼ ì¼ì£¼ì¼)
    - GET /scrape?username=zuck&max_total_posts=50  (ì´ 50ê°œë¡œ ì œí•œ)
    
    ì‘ë‹µ í˜•ì‹:
    - ë°°ì—´ì˜ ì²« ë²ˆì§¸ ìš”ì†Œ: ë©”íƒ€ë°ì´í„° (_type: "metadata")
    - ë°°ì—´ì˜ ë‘ ë²ˆì§¸ ìš”ì†Œ: ê¸°ì¡´ ì‘ë‹µ ë°ì´í„°
    """
    try:
        import time
        scrape_start_time = time.time()
        
        username = username.lstrip("@")
        
        if not username:
            raise HTTPException(status_code=400, detail="ì‚¬ìš©ìëª…ì´ í•„ìš”í•©ë‹ˆë‹¤")
        
        # 1ë‹¨ê³„: í”„ë¡œí•„ì—ì„œ ê²Œì‹œë¬¼ ëª©ë¡ ë¨¼ì € ìˆ˜ì§‘ (ë‚ ì§œ ì •ë³´ í¬í•¨)
        cutoff_utc = compute_cutoff_utc(since_days=since_days, since_date=since_date)
        scrape_result = await scrape_threads_profile(
            username=username,
            max_posts=None,  # ì œí•œ ì—†ìŒ
            max_scroll_rounds=200,
            cutoff_utc=cutoff_utc,
        )
        profile_posts = scrape_result["posts"]
        profile_scroll_rounds = scrape_result["scroll_rounds"]
        profile_duration = scrape_result["duration_seconds"]
        
        total_before_filter = len(profile_posts)
        print(f"[API] í”„ë¡œí•„ì—ì„œ ìˆ˜ì§‘ëœ ê²Œì‹œë¬¼: {total_before_filter}ê°œ")
        
        # 2ë‹¨ê³„: ë‚ ì§œ í•„í„° ì ìš© (since_dateê°€ ìˆìœ¼ë©´ ìš°ì„ )
        if since_date:
            filtered_posts = filter_posts_by_date(profile_posts, since_date=since_date)
            filter_desc = f"{since_date} ì´í›„"
        else:
            filtered_posts = filter_posts_by_date(profile_posts, since_days=since_days)
            filter_desc = f"ìµœê·¼ {since_days}ì¼ ì´ë‚´"
        
        print(f"[API] ë‚ ì§œ í•„í„°ë§ í›„: {len(filtered_posts)}ê°œ ({filter_desc})")
        
        # 3ë‹¨ê³„: ë‹µê¸€ í¬í•¨ ì‹œ í•„í„°ë§ëœ ê²Œì‹œë¬¼ì— ëŒ€í•´ì„œë§Œ ë‹µê¸€ ìˆ˜ì§‘
        if include_replies:
            import asyncio as _asyncio
            
            def convert_replies(replies: List[Dict]) -> List[ReplyPost]:
                result = []
                for r in replies:
                    result.append(ReplyPost(
                        post_id=r.get("post_id"),
                        text=r.get("text", ""),
                        created_at=r.get("created_at"),
                        url=r.get("url", ""),
                        author=r.get("author"),
                        media=r.get("media", []),
                        replies=convert_replies(r.get("replies", [])),
                    ))
                return result
            
            thread_responses = []
            total_collected = 0
            seen_root_post_ids: set[str] = set()
            
            for i, post in enumerate(filtered_posts):
                # ìµœëŒ€ ê²Œì‹œë¬¼ ìˆ˜ ë„ë‹¬ ì‹œ ì¤‘ë‹¨
                if total_collected >= max_total_posts:
                    print(f"[API] ìµœëŒ€ ê²Œì‹œë¬¼ ìˆ˜({max_total_posts}) ë„ë‹¬, ìˆ˜ì§‘ ì¤‘ë‹¨")
                    break
                
                post_url = post.get("url", "")
                if not post_url:
                    continue
                
                print(f"[API] ë‹µê¸€ ìˆ˜ì§‘ ì¤‘: {i+1}/{len(filtered_posts)} - {post_url}")
                
                try:
                    # ë‚¨ì€ ìˆ˜ì§‘ ê°€ëŠ¥ ê°œìˆ˜ ê³„ì‚°
                    remaining = max_total_posts - total_collected
                    
                    thread_data = await scrape_thread_with_replies(
                        post_url=post_url,
                        max_depth=max_reply_depth,
                        max_total_posts=remaining,
                    )

                    # ê°™ì€ ìŠ¤ë ˆë“œ(ê°™ì€ root post_id) ì¤‘ë³µ ì œê±°:
                    # í”„ë¡œí•„ì—ëŠ” ë£¨íŠ¸ ê¸€ê³¼ reply ê¸€ì´ ëª¨ë‘ ì„ì—¬ ë‚˜ì˜¬ ìˆ˜ ìˆì–´,
                    # reply URLë¡œ ìš”ì²­í•´ë„ scrape_thread_with_replies()ê°€ ë£¨íŠ¸ë¡œ ì •ê·œí™”í•œ ë’¤
                    # root post_id ê¸°ì¤€ìœ¼ë¡œ 1ë²ˆë§Œ í¬í•¨í•œë‹¤.
                    root_post_id = thread_data.get("post_id")
                    if root_post_id and root_post_id in seen_root_post_ids:
                        print(f"[API] ì¤‘ë³µ ìŠ¤ë ˆë“œ ìŠ¤í‚µ: root post_id={root_post_id} (url={post_url})")
                        continue
                    if root_post_id:
                        seen_root_post_ids.add(root_post_id)
                    
                    # ì›ë³¸ created_at ìœ ì§€ (í”„ë¡œí•„ì—ì„œ ê°€ì ¸ì˜¨ ì •í™•í•œ ë‚ ì§œ)
                    thread_data["created_at"] = post.get("created_at")
                    
                    replies_count = len(thread_data.get("replies", []))
                    thread_total = 1 + replies_count
                    total_collected += thread_total
                    
                    thread_responses.append(ThreadWithRepliesResponse(
                        post_id=thread_data.get("post_id"),
                        text=thread_data.get("text", ""),
                        created_at=thread_data.get("created_at"),
                        url=thread_data.get("url", ""),
                        author=thread_data.get("author"),
                        media=thread_data.get("media", []),
                        replies=convert_replies(thread_data.get("replies", [])),
                        total_replies_count=thread_data.get("total_replies_count", 0),
                        scraped_at=datetime.now(KST).isoformat(),
                    ))
                    
                    print(f"[API] ìŠ¤ë ˆë“œ ìˆ˜ì§‘ ì™„ë£Œ: root 1ê°œ + ë‹µê¸€ {replies_count}ê°œ (ëˆ„ì  {total_collected}ê°œ)")
                    
                except Exception as e:
                    print(f"[API] ë‹µê¸€ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
                    # ë‹µê¸€ ìˆ˜ì§‘ ì‹¤íŒ¨ ì‹œ ì›ë³¸ ê²Œì‹œë¬¼ë§Œ ì¶”ê°€
                    thread_responses.append(ThreadWithRepliesResponse(
                        post_id=None,
                        text=post.get("text", ""),
                        created_at=post.get("created_at"),
                        url=post_url,
                        author=None,
                        media=post.get("media", []),
                        replies=[],
                        total_replies_count=0,
                        scraped_at=datetime.now(KST).isoformat(),
                    ))
                    total_collected += 1
                
                # ìš”ì²­ ê°„ ê°„ê²© (rate limiting ë°©ì§€)
                await _asyncio.sleep(2)
            
            print(f"[API] ì „ì²´ ìˆ˜ì§‘ ì™„ë£Œ: ì´ {total_collected}ê°œ ê²Œì‹œë¬¼")
            
            # ë©”íƒ€ë°ì´í„° ê³„ì‚°
            total_scrape_duration = round(time.time() - scrape_start_time, 2)
            total_replies_collected = sum(t.total_replies_count for t in thread_responses)
            posts_with_replies_count = sum(1 for t in thread_responses if t.total_replies_count > 0)
            
            # ë‚ ì§œ ë²”ìœ„ ê³„ì‚°
            post_dates = [parse_datetime(t.created_at) for t in thread_responses if t.created_at]
            post_dates = [d for d in post_dates if d is not None]
            if post_dates:
                newest_date = max(post_dates)
                oldest_date = min(post_dates)
                date_range_days = (newest_date - oldest_date).days
                newest_date_str = newest_date.isoformat()
                oldest_date_str = oldest_date.isoformat()
            else:
                date_range_days = 0
                newest_date_str = oldest_date_str = None
            
            # ìƒíƒœ ê²°ì •
            if len(thread_responses) == 0:
                scrape_status = "failed"
            elif total_collected < max_total_posts and len(filtered_posts) > len(thread_responses):
                scrape_status = "partial"
            else:
                scrape_status = "success"
            
            # ê¹”ë”í•œ ë‹¨ì¼ ê°ì²´ ì‘ë‹µ
            response = {
                "meta": {
                    "username": username,
                    "scraped_at": datetime.now(KST).isoformat(),
                    "duration_seconds": total_scrape_duration,
                    "scroll_rounds": profile_scroll_rounds,
                    "status": scrape_status,
                },
                "stats": {
                    "total_scraped": total_before_filter,
                    "filtered_count": len(thread_responses),
                    "excluded_count": total_before_filter - len(filtered_posts),
                    "total_replies": total_replies_collected,
                    "posts_with_replies": posts_with_replies_count,
                },
                "date_range": {
                    "days": date_range_days,
                    "newest": newest_date_str,
                    "oldest": oldest_date_str,
                },
                "filter": {
                    "type": filter_desc,
                    "cutoff_date": cutoff_utc.isoformat() if cutoff_utc else None,
                },
                "posts": [t.model_dump() for t in thread_responses],
            }
            
            return JSONResponse(content=response)
        else:
            # ë‹µê¸€ ë¯¸í¬í•¨ ì‹œ
            post_responses = []
            for post in filtered_posts:
                try:
                    post_responses.append({
                        "text": post.get("text") or "",
                        "created_at": post.get("created_at"),
                        "url": post.get("url") or "",
                        "media": post.get("media", []),
                    })
                except Exception as post_error:
                    print(f"PostResponse ë³€í™˜ ì‹¤íŒ¨: {post_error}")
                    continue
            
            # ë©”íƒ€ë°ì´í„° ê³„ì‚°
            total_scrape_duration = round(time.time() - scrape_start_time, 2)
            
            # ë‚ ì§œ ë²”ìœ„ ê³„ì‚°
            post_dates = [parse_datetime(p["created_at"]) for p in post_responses if p.get("created_at")]
            post_dates = [d for d in post_dates if d is not None]
            if post_dates:
                newest_date = max(post_dates)
                oldest_date = min(post_dates)
                date_range_days = (newest_date - oldest_date).days
                newest_date_str = newest_date.isoformat()
                oldest_date_str = oldest_date.isoformat()
            else:
                date_range_days = 0
                newest_date_str = oldest_date_str = None
            
            # ìƒíƒœ ê²°ì •
            if len(post_responses) == 0:
                scrape_status = "failed" if total_before_filter == 0 else "success"
            else:
                scrape_status = "success"
            
            # ê¹”ë”í•œ ë‹¨ì¼ ê°ì²´ ì‘ë‹µ
            response = {
                "meta": {
                    "username": username,
                    "scraped_at": datetime.now(KST).isoformat(),
                    "duration_seconds": total_scrape_duration,
                    "scroll_rounds": profile_scroll_rounds,
                    "status": scrape_status,
                },
                "stats": {
                    "total_scraped": total_before_filter,
                    "filtered_count": len(post_responses),
                    "excluded_count": total_before_filter - len(post_responses),
                    "total_replies": 0,
                    "posts_with_replies": 0,
                },
                "date_range": {
                    "days": date_range_days,
                    "newest": newest_date_str,
                    "oldest": oldest_date_str,
                },
                "filter": {
                    "type": filter_desc,
                    "cutoff_date": cutoff_utc.isoformat() if cutoff_utc else None,
                },
                "posts": post_responses,
            }
            
            return JSONResponse(content=response)
    except HTTPException:
        raise
    except Exception as e:
        error_detail = f"ìŠ¤í¬ë˜í•‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        print(f"ìŠ¤í¬ë˜í•‘ ì˜¤ë¥˜ ìƒì„¸:")
        print(traceback.format_exc())
        raise HTTPException(status_code=500, detail=error_detail)


@app.post("/scrape", response_model=ScrapeResponse)
async def scrape_post(request: ScrapeRequest):
    """POST ë°©ì‹ìœ¼ë¡œ ìŠ¤í¬ë˜í•‘ ìš”ì²­ (ì™¸ë¶€ ì‚¬ìš©ììš©)
    
    ì˜ˆì‹œ:
    ```json
    {
        "username": "zuck",
        "max_posts": 10,
        "since_days": 7
    }
    ```
    """
    try:
        username = request.username.lstrip("@")
        
        if not username:
            raise HTTPException(status_code=400, detail="ì‚¬ìš©ìëª…ì´ í•„ìš”í•©ë‹ˆë‹¤")
        
        scrape_result = await scrape_threads_profile(
            username=username,
            max_posts=request.max_posts,
            max_scroll_rounds=200,  # ë‚´ë¶€ì ìœ¼ë¡œ ì¶©ë¶„íˆ ë†’ì€ ê°’ ì‚¬ìš©
            cutoff_utc=compute_cutoff_utc(since_days=request.since_days, since_date=request.since_date),
        )
        posts = scrape_result["posts"]
        
        total_before_filter = len(posts)
        
        # ë‚ ì§œ í•„í„° ì ìš©
        filtered_posts = filter_posts_by_date(posts, since_days=request.since_days, since_date=request.since_date)
        
        # í•„í„° ì„¤ëª… ìƒì„±
        filter_desc = None
        if request.since_days:
            filter_desc = f"ìµœê·¼ {request.since_days}ì¼ ì´ë‚´"
        elif request.since_date:
            filter_desc = f"{request.since_date} ì´í›„"
        
        # PostResponse ìƒì„± ì‹œ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
        post_responses = []
        for post in filtered_posts:
            try:
                # í•„ìˆ˜ í•„ë“œ ê²€ì¦ ë° ê¸°ë³¸ê°’ ì„¤ì •
                post_data = {
                    "text": post.get("text") or "",
                    "created_at": post.get("created_at"),
                    "url": post.get("url") or "",
                    "media": post.get("media", []),
                }
                post_responses.append(PostResponse(**post_data))
            except Exception as post_error:
                # ê°œë³„ post ë³€í™˜ ì‹¤íŒ¨ëŠ” ë¡œê·¸ë§Œ ë‚¨ê¸°ê³  ê±´ë„ˆëœ€
                print(f"PostResponse ë³€í™˜ ì‹¤íŒ¨: {post_error}")
                print(f"Post ë°ì´í„°: {post}")
                print(traceback.format_exc())
                continue
        
        return ScrapeResponse(
            username=username,
            total_posts=total_before_filter,
            filtered_posts=len(post_responses),
            posts=post_responses,
            scraped_at=datetime.now(KST).isoformat(),
            filter_applied=filter_desc,
        )
    except HTTPException:
        raise
    except Exception as e:
        error_detail = f"ìŠ¤í¬ë˜í•‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        print(f"ìŠ¤í¬ë˜í•‘ ì˜¤ë¥˜ ìƒì„¸:")
        print(traceback.format_exc())
        raise HTTPException(status_code=500, detail=error_detail)


@app.post("/v2/scrape")
async def scrape_v2(request: V2ScrapeRequest):
    """ìƒˆë¡œìš´ ì‘ë‹µ í˜•ì‹ì˜ ìŠ¤í¬ë˜í•‘ ì—”ë“œí¬ì¸íŠ¸ (POST ì „ìš©)
    
    ìŠ¤íƒ€ì¼:
    - threaded: ë£¨íŠ¸ ìŠ¤ë ˆë“œ ë‹¨ìœ„ë¡œ replies í¬í•¨
    - flat: ëª¨ë“  ê²Œì‹œë¬¼ì„ í‰íƒ„í™”í•˜ì—¬ ë°˜í™˜
    """
    try:
        username = request.username.lstrip("@")
        if not username:
            raise HTTPException(status_code=400, detail="ì‚¬ìš©ìëª…ì´ í•„ìš”í•©ë‹ˆë‹¤")
        
        # ê¸°ì¡´ ë¡œì§ ì¬ì‚¬ìš©
        result = await scrape_single_account_v2(
            username=username,
            max_posts=None,
            max_scroll_rounds=200,
            since_days=request.since_days,
            since_date=request.since_date,
            include_replies=request.include_replies,
            max_reply_depth=request.max_reply_depth,
            max_total_posts=request.max_total_posts,
            semaphore=None,
            shared_counter=None,
        )
        
        response = {
            "meta": {
                "username": result["meta"]["username"],
                "scraped_at": result["meta"]["scraped_at"],
                "duration_seconds": result["meta"]["duration_seconds"],
                "scroll_rounds": result["meta"]["scroll_rounds"],
                "status": result["meta"]["status"],
            },
            "filter": {
                "type": result["filter"]["type"],
                "cutoff_date": result["filter"]["cutoff_date"],
            },
            "stats": {
                "total_scraped": result["stats"]["total_scraped"],
                "filtered_count": result["stats"]["filtered_count"],
                "excluded_count": result["stats"]["excluded_count"],
                "total_replies": result["stats"]["total_replies"],
                "posts_with_replies": result["stats"]["posts_with_replies"],
            },
            "request": {
                "since_days": request.since_days,
                "since_date": request.since_date,
                "include_replies": request.include_replies,
                "max_reply_depth": request.max_reply_depth,
                "max_total_posts": request.max_total_posts,
                "response_style": request.response_style,
            },
            "data": build_v2_data_from_result(
                result=result,
                response_style=request.response_style,
                include_replies=request.include_replies,
            ),
        }
        
        return JSONResponse(content=response)
    except HTTPException:
        raise
    except Exception as e:
        error_detail = f"ìŠ¤í¬ë˜í•‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        print(f"v2 ìŠ¤í¬ë˜í•‘ ì˜¤ë¥˜ ìƒì„¸:")
        print(traceback.format_exc())
        raise HTTPException(status_code=500, detail=error_detail)

class SharedCounter:
    """ìŠ¤ë ˆë“œ ì•ˆì „í•œ ê³µìœ  ì¹´ìš´í„° (ë³‘ë ¬ ìŠ¤í¬ë˜í•‘ì—ì„œ max_total ì œì–´ìš©)"""
    def __init__(self, max_total: int):
        self.max_total = max_total
        self.count = 0
        self._lock = asyncio.Lock()
    
    async def try_add(self, amount: int) -> int:
        """
        ì§€ì •ëœ ì–‘ì„ ì¶”ê°€ ì‹œë„í•˜ê³  ì‹¤ì œë¡œ ì¶”ê°€ëœ ì–‘ì„ ë°˜í™˜.
        max_totalì„ ì´ˆê³¼í•˜ë©´ ê°€ëŠ¥í•œ ì–‘ë§Œ ì¶”ê°€.
        """
        async with self._lock:
            remaining = self.max_total - self.count
            if remaining <= 0:
                return 0
            added = min(amount, remaining)
            self.count += added
            return added
    
    async def get_remaining(self) -> int:
        """ë‚¨ì€ ìˆ˜ì§‘ ê°€ëŠ¥ ê°œìˆ˜ ë°˜í™˜"""
        async with self._lock:
            return max(0, self.max_total - self.count)
    
    async def is_full(self) -> bool:
        """max_totalì— ë„ë‹¬í–ˆëŠ”ì§€ í™•ì¸"""
        async with self._lock:
            return self.count >= self.max_total


async def scrape_single_account_v2(
    username: str,
    max_posts: Optional[int],
    max_scroll_rounds: int,
    since_days: Optional[int] = None,
    since_date: Optional[str] = None,
    include_replies: bool = True,
    max_reply_depth: int = 1,
    max_total_posts: int = 100,
    semaphore: Optional[asyncio.Semaphore] = None,
    shared_counter: Optional[SharedCounter] = None,
) -> Dict[str, Any]:
    """ë‹¨ì¼ ê³„ì • ìŠ¤í¬ë˜í•‘ í—¬í¼ í•¨ìˆ˜ (GET /scrapeì™€ ë™ì¼í•œ ë¡œì§)
    
    shared_counterê°€ ì œê³µë˜ë©´ ì „ì²´ max_totalì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ì²´í¬í•˜ì—¬
    ë„ë‹¬ ì‹œ ì¡°ê¸° ì¢…ë£Œí•©ë‹ˆë‹¤.
    """
    import time
    start_time = time.time()
    username = username.lstrip("@")
    cutoff_utc = compute_cutoff_utc(since_days=since_days, since_date=since_date)
    
    # í•„í„° ì„¤ëª… ìƒì„±
    if since_days:
        filter_desc = f"ìµœê·¼ {since_days}ì¼ ì´ë‚´"
    elif since_date:
        filter_desc = f"{since_date} ì´í›„"
    else:
        filter_desc = None
    
    async def do_scrape():
        # ì‹œì‘ ì „ì— ì´ë¯¸ max_total ë„ë‹¬í–ˆëŠ”ì§€ í™•ì¸
        if shared_counter and await shared_counter.is_full():
            print(f"[batch] @{username}: max_total ì´ë¯¸ ë„ë‹¬, ìŠ¤í‚µ")
            return {
                "meta": {
                    "username": username,
                    "scraped_at": datetime.now(KST).isoformat(),
                    "duration_seconds": 0,
                    "scroll_rounds": 0,
                    "status": "skipped",
                    "reason": "max_total reached",
                },
                "stats": {
                    "total_scraped": 0,
                    "filtered_count": 0,
                    "excluded_count": 0,
                    "total_replies": 0,
                    "posts_with_replies": 0,
                },
                "date_range": {"days": 0, "newest": None, "oldest": None},
                "filter": {"type": filter_desc, "cutoff_date": cutoff_utc.isoformat() if cutoff_utc else None},
                "posts": [],
            }
        
        if not username:
            return {
                "meta": {
                    "username": username,
                    "scraped_at": datetime.now(KST).isoformat(),
                    "duration_seconds": 0,
                    "scroll_rounds": 0,
                    "status": "failed",
                    "error": "ì‚¬ìš©ìëª…ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤",
                },
                "stats": {
                    "total_scraped": 0,
                    "filtered_count": 0,
                    "excluded_count": 0,
                    "total_replies": 0,
                    "posts_with_replies": 0,
                },
                "date_range": {"days": 0, "newest": None, "oldest": None},
                "filter": {"type": filter_desc, "cutoff_date": cutoff_utc.isoformat() if cutoff_utc else None},
                "posts": [],
            }
        
        try:
            scrape_result = await scrape_threads_profile(
                username=username,
                max_posts=max_posts,
                max_scroll_rounds=max_scroll_rounds,
                cutoff_utc=cutoff_utc,
            )
            posts = scrape_result["posts"]
            scroll_rounds = scrape_result["scroll_rounds"]
            
            total_before_filter = len(posts)
            
            # ë‚ ì§œ í•„í„° ì ìš©
            filtered_posts = filter_posts_by_date(posts, since_days=since_days, since_date=since_date)
            
            # include_repliesì— ë”°ë¼ ì²˜ë¦¬
            if include_replies:
                # ë‹µê¸€ í¬í•¨ - GET /scrapeì™€ ë™ì¼í•œ ë¡œì§
                post_responses = []
                total_collected = 0
                total_replies_collected = 0
                posts_with_replies_count = 0
                seen_root_post_ids: set = set()
                
                for post in filtered_posts:
                    # ê³µìœ  ì¹´ìš´í„°ë¡œ max_total ì²´í¬
                    if shared_counter and await shared_counter.is_full():
                        print(f"[batch] @{username}: ìŠ¤í¬ë˜í•‘ ì¤‘ max_total ë„ë‹¬, ì¡°ê¸° ì¢…ë£Œ")
                        break
                    
                    # ë¡œì»¬ max_total_posts ì²´í¬
                    if total_collected >= max_total_posts:
                        break
                    
                    post_url = post.get("url", "")
                    if not post_url:
                        continue
                    
                    try:
                        remaining = max_total_posts - total_collected
                        thread_data = await scrape_thread_with_replies(
                            post_url=post_url,
                            max_depth=max_reply_depth,
                            max_total_posts=remaining,
                        )
                        
                        root_post_id = thread_data.get("post_id")
                        if root_post_id and root_post_id in seen_root_post_ids:
                            continue
                        if root_post_id:
                            seen_root_post_ids.add(root_post_id)
                        
                        thread_data["created_at"] = post.get("created_at")
                        replies_count = len(thread_data.get("replies", []))
                        post_total = 1 + replies_count
                        
                        # ê³µìœ  ì¹´ìš´í„°ì— ì¶”ê°€ ì‹œë„
                        if shared_counter:
                            added = await shared_counter.try_add(post_total)
                            if added == 0:
                                print(f"[batch] @{username}: ê³µìœ  ì¹´ìš´í„° ê°€ë“ ì°¸, ì¡°ê¸° ì¢…ë£Œ")
                                break
                            # ë¶€ë¶„ì ìœ¼ë¡œë§Œ ì¶”ê°€ëœ ê²½ìš°ë„ ì¼ë‹¨ í¬í•¨ (ì´ë¯¸ ìˆ˜ì§‘ë¨)
                        
                        total_collected += post_total
                        total_replies_collected += replies_count
                        if replies_count > 0:
                            posts_with_replies_count += 1
                        
                        post_responses.append(thread_data)
                        
                    except Exception as e:
                        # ê³µìœ  ì¹´ìš´í„°ì— 1 ì¶”ê°€
                        if shared_counter:
                            added = await shared_counter.try_add(1)
                            if added == 0:
                                break
                        
                        post_responses.append({
                            "post_id": None,
                            "text": post.get("text", ""),
                            "created_at": post.get("created_at"),
                            "url": post_url,
                            "author": None,
                            "replies": [],
                            "total_replies_count": 0,
                        })
                        total_collected += 1
                    
                    await asyncio.sleep(1)  # rate limiting
            else:
                # ë‹µê¸€ ë¯¸í¬í•¨
                post_responses = []
                total_replies_collected = 0
                posts_with_replies_count = 0
                if any("is_reply" in p for p in filtered_posts):
                    filtered_posts = [p for p in filtered_posts if not p.get("is_reply")]
                
                for post in filtered_posts[:max_total_posts]:
                    # ê³µìœ  ì¹´ìš´í„° ì²´í¬
                    if shared_counter and await shared_counter.is_full():
                        break
                    
                    try:
                        # ê³µìœ  ì¹´ìš´í„°ì— 1 ì¶”ê°€
                        if shared_counter:
                            added = await shared_counter.try_add(1)
                            if added == 0:
                                break
                        
                        post_responses.append({
                            "text": post.get("text") or "",
                            "created_at": post.get("created_at"),
                            "url": post.get("url") or "",
                        })
                    except Exception:
                        continue
            
            duration = round(time.time() - start_time, 2)
            
            # ë‚ ì§œ ë²”ìœ„ ê³„ì‚°
            post_dates = [parse_datetime(p["created_at"]) for p in post_responses if p.get("created_at")]
            post_dates = [d for d in post_dates if d is not None]
            if post_dates:
                newest_date = max(post_dates)
                oldest_date = min(post_dates)
                date_range_days = (newest_date - oldest_date).days
                newest_date_str = newest_date.isoformat()
                oldest_date_str = oldest_date.isoformat()
            else:
                date_range_days = 0
                newest_date_str = oldest_date_str = None
            
            # ìƒíƒœ ê²°ì •
            if len(post_responses) == 0:
                status = "failed" if total_before_filter == 0 else "success"
            else:
                status = "success"
            
            return {
                "meta": {
                    "username": username,
                    "scraped_at": datetime.now(KST).isoformat(),
                    "duration_seconds": duration,
                    "scroll_rounds": scroll_rounds,
                    "status": status,
                },
                "stats": {
                    "total_scraped": total_before_filter,
                    "filtered_count": len(post_responses),
                    "excluded_count": total_before_filter - len(filtered_posts),
                    "total_replies": total_replies_collected,
                    "posts_with_replies": posts_with_replies_count,
                },
                "date_range": {
                    "days": date_range_days,
                    "newest": newest_date_str,
                    "oldest": oldest_date_str,
                },
                "filter": {
                    "type": filter_desc,
                    "cutoff_date": cutoff_utc.isoformat() if cutoff_utc else None,
                },
                "posts": post_responses,
            }
        except Exception as e:
            duration = round(time.time() - start_time, 2)
            return {
                "meta": {
                    "username": username,
                    "scraped_at": datetime.now(KST).isoformat(),
                    "duration_seconds": duration,
                    "scroll_rounds": 0,
                    "status": "failed",
                    "error": str(e),
                },
                "stats": {
                    "total_scraped": 0,
                    "filtered_count": 0,
                    "excluded_count": 0,
                    "total_replies": 0,
                    "posts_with_replies": 0,
                },
                "date_range": {"days": 0, "newest": None, "oldest": None},
                "filter": {"type": filter_desc, "cutoff_date": cutoff_utc.isoformat() if cutoff_utc else None},
                "posts": [],
            }
    
    # Semaphoreë¡œ ë™ì‹œì„± ì œí•œ
    if semaphore:
        async with semaphore:
            print(f"[batch] ìŠ¤í¬ë˜í•‘ ì‹œì‘: @{username}")
            result = await do_scrape()
            print(f"[batch] ìŠ¤í¬ë˜í•‘ ì™„ë£Œ: @{username} ({result['meta']['duration_seconds']}ì´ˆ, {result['stats']['filtered_count']}ê°œ)")
            return result
    else:
        return await do_scrape()


async def scrape_single_account(
    username: str,
    max_posts: Optional[int],
    max_scroll_rounds: int,
    since_days: Optional[int] = None,
    since_date: Optional[str] = None,
) -> BatchScrapeItem:
    """ë‹¨ì¼ ê³„ì • ìŠ¤í¬ë˜í•‘ í—¬í¼ í•¨ìˆ˜ (ë ˆê±°ì‹œ í˜¸í™˜ìš©)"""
    username = username.lstrip("@")
    scraped_at = datetime.now(KST).isoformat()
    
    try:
        if not username:
            return BatchScrapeItem(
                username=username,
                success=False,
                total_posts=0,
                filtered_posts=0,
                posts=[],
                error="ì‚¬ìš©ìëª…ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤",
                scraped_at=scraped_at,
            )
        
        scrape_result = await scrape_threads_profile(
            username=username,
            max_posts=max_posts,
            max_scroll_rounds=max_scroll_rounds,
            cutoff_utc=compute_cutoff_utc(since_days=since_days, since_date=since_date),
        )
        posts = scrape_result["posts"]
        
        total_before_filter = len(posts)
        
        # ë‚ ì§œ í•„í„° ì ìš©
        filtered_posts = filter_posts_by_date(posts, since_days=since_days, since_date=since_date)
        
        # í•„í„° ì„¤ëª… ìƒì„±
        filter_desc = None
        if since_days:
            filter_desc = f"ìµœê·¼ {since_days}ì¼ ì´ë‚´"
        elif since_date:
            filter_desc = f"{since_date} ì´í›„"
        
        # PostResponse ìƒì„± ì‹œ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
        post_responses = []
        for post in filtered_posts:
            try:
                # í•„ìˆ˜ í•„ë“œ ê²€ì¦ ë° ê¸°ë³¸ê°’ ì„¤ì •
                post_data = {
                    "text": post.get("text") or "",
                    "created_at": post.get("created_at"),
                    "url": post.get("url") or "",
                    "media": post.get("media", []),
                }
                post_responses.append(PostResponse(**post_data))
            except Exception as post_error:
                # ê°œë³„ post ë³€í™˜ ì‹¤íŒ¨ëŠ” ë¡œê·¸ë§Œ ë‚¨ê¸°ê³  ê±´ë„ˆëœ€
                print(f"PostResponse ë³€í™˜ ì‹¤íŒ¨: {post_error}")
                print(f"Post ë°ì´í„°: {post}")
                print(traceback.format_exc())
                continue
        
        return BatchScrapeItem(
            username=username,
            success=True,
            total_posts=total_before_filter,
            filtered_posts=len(post_responses),
            posts=post_responses,
            error=None,
            scraped_at=scraped_at,
            filter_applied=filter_desc,
        )
    except Exception as e:
        return BatchScrapeItem(
            username=username,
            success=False,
            total_posts=0,
            filtered_posts=0,
            posts=[],
            error=str(e),
            scraped_at=scraped_at,
        )


@app.post("/internal/batch-scrape", response_model=BatchScrapeResponse)
async def batch_scrape_legacy(request: BatchScrapeRequest):
    """ë°°ì¹˜ ìŠ¤í¬ë˜í•‘ ìš”ì²­ (ë ˆê±°ì‹œ í˜¸í™˜ìš©)"""
    try:
        if not request.usernames:
            raise HTTPException(status_code=400, detail="ì‚¬ìš©ìëª… ë¦¬ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")
        
        unique_usernames = list(dict.fromkeys(request.usernames))
        
        tasks = [
            scrape_single_account(
                username=username,
                max_posts=request.max_posts,
                max_scroll_rounds=request.max_scroll_rounds,
                since_days=request.since_days,
                since_date=request.since_date,
            )
            for username in unique_usernames
        ]
        
        results = await asyncio.gather(*tasks)
        
        successful_count = sum(1 for r in results if r.success)
        failed_count = len(results) - successful_count
        
        return BatchScrapeResponse(
            total_accounts=len(results),
            successful_accounts=successful_count,
            failed_accounts=failed_count,
            results=results,
            completed_at=datetime.now(KST).isoformat(),
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ë°°ì¹˜ ìŠ¤í¬ë˜í•‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")


@app.post("/batch-scrape")
async def batch_scrape_v2(request: BatchScrapeRequestV2):
    """ë°°ì¹˜ ìŠ¤í¬ë˜í•‘ - ëª¨ë“  ê³„ì • ë³‘ë ¬ ì²˜ë¦¬ + ì „ì²´ max_total ì‹¤ì‹œê°„ ì œì–´
    
    ì—¬ëŸ¬ ê³„ì •ì„ í•œ ë²ˆì— ìŠ¤í¬ë˜í•‘í•©ë‹ˆë‹¤.
    - ëª¨ë“  ê³„ì •ì„ ë™ì‹œì— ë³‘ë ¬ ì²˜ë¦¬ (ìµœëŒ€ 5ê°œ ë™ì‹œ ì‹¤í–‰)
    - max_per_account: ê³„ì •ë³„ ìµœëŒ€ ìŠ¤í¬ë© ê°¯ìˆ˜
    - max_total: ì „ì²´ ìµœëŒ€ ê°¯ìˆ˜ - ì‹¤ì‹œê°„ìœ¼ë¡œ ì²´í¬í•˜ì—¬ ë„ë‹¬ ì‹œ ì¡°ê¸° ì¢…ë£Œ
    
    íŒŒë¼ë¯¸í„°:
    - usernames: ì‚¬ìš©ìëª… ë¦¬ìŠ¤íŠ¸ (í•„ìˆ˜)
    - since_days: ìµœê·¼ Nì¼ ì´ë‚´ (ê¸°ë³¸: 1ì¼)
    - max_per_account: ê³„ì •ë³„ ìµœëŒ€ ìŠ¤í¬ë© ê°¯ìˆ˜ (ê¸°ë³¸: 10)
    - max_total: ì „ì²´ ìµœëŒ€ ìŠ¤í¬ë© ê°¯ìˆ˜ (ê¸°ë³¸: 300, ì‹¤ì‹œê°„ ì²´í¬)
    - include_replies: ë‹µê¸€ í¬í•¨ ì—¬ë¶€ (ê¸°ë³¸: True)
    - max_reply_depth: ë‹µê¸€ ê¹Šì´ (ê¸°ë³¸: 1)
    
    ì˜ˆì‹œ:
    ```json
    {
        "usernames": ["zuck", "meta", "instagram"],
        "max_per_account": 10,
        "max_total": 300
    }
    ```
    """
    import time
    batch_start_time = time.time()
    
    try:
        if not request.usernames:
            raise HTTPException(status_code=400, detail="ì‚¬ìš©ìëª… ë¦¬ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")
        
        unique_usernames = list(dict.fromkeys(request.usernames))
        
        print(f"[batch] ğŸš€ ë³‘ë ¬ ë°°ì¹˜ ìŠ¤í¬ë˜í•‘ ì‹œì‘: {len(unique_usernames)}ê°œ ê³„ì •")
        print(f"[batch] ì„¤ì •: max_per_account={request.max_per_account}, max_total={request.max_total}, ë™ì‹œì²˜ë¦¬=5")
        
        # ê³µìœ  ì¹´ìš´í„° ìƒì„± (max_total ì‹¤ì‹œê°„ ì œì–´ìš©)
        shared_counter = SharedCounter(max_total=request.max_total)
        
        # ì„¸ë§ˆí¬ì–´ë¡œ ë™ì‹œì„± 5ê°œ ì œí•œ
        semaphore = asyncio.Semaphore(5)
        
        # ëª¨ë“  ê³„ì •ì— ëŒ€í•´ ë³‘ë ¬ íƒœìŠ¤í¬ ìƒì„±
        tasks = [
            scrape_single_account_v2(
                username=username,
                max_posts=request.max_per_account,
                max_scroll_rounds=50,
                since_days=request.since_days,
                since_date=request.since_date,
                include_replies=request.include_replies,
                max_reply_depth=request.max_reply_depth,
                max_total_posts=request.max_per_account,
                semaphore=semaphore,
                shared_counter=shared_counter,
            )
            for username in unique_usernames
        ]
        
        # ëª¨ë“  íƒœìŠ¤í¬ ë³‘ë ¬ ì‹¤í–‰
        results = await asyncio.gather(*tasks)
        
        # ì „ì²´ ë©”íƒ€ë°ì´í„° ê³„ì‚°
        batch_duration = round(time.time() - batch_start_time, 2)
        success_count = sum(1 for r in results if r["meta"]["status"] == "success")
        skipped_count = sum(1 for r in results if r["meta"]["status"] == "skipped")
        failed_count = sum(1 for r in results if r["meta"]["status"] == "failed")
        total_scraped = sum(r["stats"]["total_scraped"] for r in results)
        total_filtered = sum(r["stats"]["filtered_count"] for r in results)
        total_excluded = sum(r["stats"]["excluded_count"] for r in results)
        total_replies = sum(r["stats"]["total_replies"] for r in results)
        
        # ìŠ¤í‚µëœ ê³„ì • ëª©ë¡
        skipped_usernames = [r["meta"]["username"] for r in results if r["meta"]["status"] == "skipped"]
        
        # ì „ì²´ ë‚ ì§œ ë²”ìœ„ ê³„ì‚°
        all_dates = []
        for r in results:
            if r["date_range"]["newest"]:
                all_dates.append(parse_datetime(r["date_range"]["newest"]))
            if r["date_range"]["oldest"]:
                all_dates.append(parse_datetime(r["date_range"]["oldest"]))
        all_dates = [d for d in all_dates if d is not None]
        
        if all_dates:
            overall_newest = max(all_dates).isoformat()
            overall_oldest = min(all_dates).isoformat()
            overall_days = (max(all_dates) - min(all_dates)).days
        else:
            overall_newest = overall_oldest = None
            overall_days = 0
        
        print(f"[batch] âœ… ë³‘ë ¬ ë°°ì¹˜ ìŠ¤í¬ë˜í•‘ ì™„ë£Œ!")
        print(f"[batch]    ì„±ê³µ: {success_count}, ìŠ¤í‚µ: {skipped_count}, ì‹¤íŒ¨: {failed_count}")
        print(f"[batch]    ì´ ìˆ˜ì§‘: {total_filtered}ê°œ, ì†Œìš”ì‹œê°„: {batch_duration}ì´ˆ")
        
        response = {
            "total_meta": {
                "accounts_requested": len(unique_usernames),
                "accounts_processed": len(results),
                "accounts_skipped": skipped_count,
                "skipped_usernames": skipped_usernames,
                "success_count": success_count,
                "failed_count": failed_count,
                "total_duration_seconds": batch_duration,
                "total_collected": total_filtered,
                "max_total_reached": shared_counter.count >= request.max_total,
                "total_replies": total_replies,
                "settings": {
                    "max_per_account": request.max_per_account,
                    "max_total": request.max_total,
                    "since_days": request.since_days,
                    "include_replies": request.include_replies,
                    "concurrency": 5,
                },
                "date_range": {
                    "days": overall_days,
                    "newest": overall_newest,
                    "oldest": overall_oldest,
                },
                "completed_at": datetime.now(KST).isoformat(),
            },
            "results": results,
        }
        
        return JSONResponse(content=response)
    except HTTPException:
        raise
    except Exception as e:
        print(f"ë°°ì¹˜ ìŠ¤í¬ë˜í•‘ ì˜¤ë¥˜:")
        print(traceback.format_exc())
        raise HTTPException(status_code=500, detail=f"ë°°ì¹˜ ìŠ¤í¬ë˜í•‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")


@app.post("/v2/batch-scrape")
async def batch_scrape_v2_clean(request: V2BatchScrapeRequest):
    """ìƒˆë¡œìš´ ì‘ë‹µ í˜•ì‹ì˜ ë°°ì¹˜ ìŠ¤í¬ë˜í•‘ ì—”ë“œí¬ì¸íŠ¸ (POST ì „ìš©)"""
    import time
    batch_start_time = time.time()
    
    try:
        if not request.usernames:
            raise HTTPException(status_code=400, detail="ì‚¬ìš©ìëª… ë¦¬ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")
        
        unique_usernames = list(dict.fromkeys(request.usernames))
        
        shared_counter = SharedCounter(max_total=request.max_total)
        semaphore = asyncio.Semaphore(5)
        
        tasks = [
            scrape_single_account_v2(
                username=username,
                max_posts=request.max_per_account,
                max_scroll_rounds=50,
                since_days=request.since_days,
                since_date=request.since_date,
                include_replies=request.include_replies,
                max_reply_depth=request.max_reply_depth,
                max_total_posts=request.max_per_account,
                semaphore=semaphore,
                shared_counter=shared_counter,
            )
            for username in unique_usernames
        ]
        
        results = await asyncio.gather(*tasks)
        
        batch_duration = round(time.time() - batch_start_time, 2)
        success_count = sum(1 for r in results if r["meta"]["status"] == "success")
        skipped_count = sum(1 for r in results if r["meta"]["status"] == "skipped")
        failed_count = sum(1 for r in results if r["meta"]["status"] == "failed")
        total_filtered = sum(r["stats"]["filtered_count"] for r in results)
        total_replies = sum(r["stats"]["total_replies"] for r in results)
        
        skipped_usernames = [r["meta"]["username"] for r in results if r["meta"]["status"] == "skipped"]
        
        # ì „ì²´ ë‚ ì§œ ë²”ìœ„ ê³„ì‚°
        all_dates = []
        for r in results:
            if r["date_range"]["newest"]:
                all_dates.append(parse_datetime(r["date_range"]["newest"]))
            if r["date_range"]["oldest"]:
                all_dates.append(parse_datetime(r["date_range"]["oldest"]))
        all_dates = [d for d in all_dates if d is not None]
        
        if all_dates:
            overall_newest = max(all_dates).isoformat()
            overall_oldest = min(all_dates).isoformat()
            overall_days = (max(all_dates) - min(all_dates)).days
        else:
            overall_newest = overall_oldest = None
            overall_days = 0
        
        cleaned_results = []
        for r in results:
            cleaned_results.append({
                "meta": r["meta"],
                "filter": r["filter"],
                "stats": r["stats"],
                "data": build_v2_data_from_result(
                    result=r,
                    response_style=request.response_style,
                    include_replies=request.include_replies,
                ),
            })
        
        response = {
            "batch_meta": {
                "accounts_requested": len(unique_usernames),
                "accounts_processed": len(results),
                "accounts_skipped": skipped_count,
                "skipped_usernames": skipped_usernames,
                "success_count": success_count,
                "failed_count": failed_count,
                "total_duration_seconds": batch_duration,
                "total_collected": total_filtered,
                "max_total_reached": shared_counter.count >= request.max_total,
                "total_replies": total_replies,
                "settings": {
                    "max_per_account": request.max_per_account,
                    "max_total": request.max_total,
                    "since_days": request.since_days,
                    "include_replies": request.include_replies,
                    "concurrency": 5,
                    "response_style": request.response_style,
                },
                "date_range": {
                    "days": overall_days,
                    "newest": overall_newest,
                    "oldest": overall_oldest,
                },
                "completed_at": datetime.now(KST).isoformat(),
            },
            "request": {
                "since_days": request.since_days,
                "since_date": request.since_date,
                "include_replies": request.include_replies,
                "max_reply_depth": request.max_reply_depth,
                "max_per_account": request.max_per_account,
                "max_total": request.max_total,
                "response_style": request.response_style,
            },
            "results": cleaned_results,
        }
        
        return JSONResponse(content=response)
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"v2 ë°°ì¹˜ ìŠ¤í¬ë˜í•‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")


if __name__ == "__main__":
    import uvicorn
    import os
    port = int(os.getenv("PORT", "8001"))
    print(f"ì„œë²„ ì‹œì‘ ì¤‘... http://0.0.0.0:{port}")
    print(f"API ë¬¸ì„œ: http://localhost:{port}/docs")
    uvicorn.run(app, host="0.0.0.0", port=port)
